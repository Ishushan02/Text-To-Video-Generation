{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4b6520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helperFunctions import VectorQuantizeImage, VecQVAE, FrameDataset\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "import urllib\n",
    "import io\n",
    "from PIL import Image, ImageSequence\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ccc12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "\n",
    "wandb.init(\n",
    "    project=\"T2V-Decoder\",  \n",
    "    name=\"experiment-1-thread-1\",    \n",
    "    # id=\"m6ms1f4w\",  \n",
    "    # resume=\"allow\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b7f0ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"/Users/ishananand/Desktop/Text-To-Video-Generation/data/modified_tgif.csv\")\n",
    "dataset = dataset[(dataset['frames'] <= 40) & (dataset['frames'] > 15)].copy().reset_index(drop=True)\n",
    "dataset = dataset[:10000] \n",
    "\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de5afaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 3, 128, 128]) a man smiles and nods his head.\n"
     ]
    }
   ],
   "source": [
    "CACHEDIR = \"/Users/ishananand/Desktop/Text-To-Video-Generation/data/cacheGIF\"\n",
    "\n",
    "class FrameDataset(Dataset):\n",
    "    def __init__(self, data, totalSequence=40, transform=None, cache_dir=CACHEDIR):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.totalSequence = totalSequence\n",
    "        self.cache_dir = cache_dir\n",
    "        os.makedirs(self.cache_dir, exist_ok=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def npArray(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        url = row['url']\n",
    "        resp = urllib.request.urlopen(url)\n",
    "        image_data = resp.read()\n",
    "        img = Image.open(io.BytesIO(image_data))\n",
    "\n",
    "        frames = []\n",
    "        for frame in ImageSequence.Iterator(img):\n",
    "            frame_rgb = frame.convert(\"RGB\")\n",
    "            frames.append(np.array(frame_rgb))\n",
    "        return frames\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        url = row['url']\n",
    "        caption = row['caption']\n",
    "        totalframes = row['frames']\n",
    "        gif_path = os.path.join(self.cache_dir, f'{index}.gif')\n",
    "\n",
    "        if not os.path.exists(gif_path):\n",
    "            resp = urllib.request.urlopen(url)\n",
    "            image_data = resp.read()\n",
    "            with open(gif_path, 'wb') as f:\n",
    "                f.write(image_data)\n",
    "        else:\n",
    "            with open(gif_path, 'rb') as f:\n",
    "                image_data = f.read()\n",
    "\n",
    "        img = Image.open(io.BytesIO(image_data))\n",
    "\n",
    "        frames = []\n",
    "        for frame in ImageSequence.Iterator(img):\n",
    "            frame_rgb = frame.convert(\"RGB\")\n",
    "            frames.append(np.array(frame_rgb))\n",
    "\n",
    "        if len(frames) < self.totalSequence:\n",
    "            frames += [frames[-1]] * (self.totalSequence - len(frames))\n",
    "        else:\n",
    "            frames = frames[:self.totalSequence]\n",
    "\n",
    "        if self.transform:\n",
    "            tensorFrames = torch.stack([\n",
    "                self.transform(Image.fromarray(frame)) for frame in frames\n",
    "            ])\n",
    "            tensorFrames = tensorFrames / 255.0\n",
    "            return tensorFrames, caption\n",
    "        else:\n",
    "            return frames, caption\n",
    "\n",
    " \n",
    "\n",
    "tranform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "fdata = FrameDataset(dataset, transform=tranform)\n",
    "\n",
    "X, Y = fdata.__getitem__(1000)\n",
    "print(X.shape, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1eb95104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([40, 256, 32, 32]),\n",
       " torch.Size([40, 3, 128, 128]),\n",
       " tensor(0.0217, grad_fn=<MseLossBackward0>),\n",
       " tensor(0.0217, grad_fn=<MseLossBackward0>),\n",
       " torch.Size([40960]),\n",
       " tensor(546.5923),\n",
       " tensor(0.0906))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codeBookdim = 1024\n",
    "embedDim = 256\n",
    "hiddenDim = 512\n",
    "inChannels = 3\n",
    "tranform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "modelA = VecQVAE(inChannels = inChannels, hiddenDim = hiddenDim, codeBookdim = codeBookdim, embedDim = embedDim).to(device)\n",
    "\n",
    "modelValA = torch.load(\"/Users/ishananand/Desktop/Text-To-Video-Generation/models/VQVAE-GIF-thread-52.pt\", map_location=torch.device('cpu'))\n",
    "epochs = 1000\n",
    "modelA.load_state_dict(modelValA['model_state_dict'])\n",
    "\n",
    "test = torch.randn(1, 40, 3, 128, 128)\n",
    "quantized_latents, decoderOut, codebook_loss, commitment_loss, encoding_indices, perplexity, diversity_loss = modelA(test)\n",
    "quantized_latents.shape, decoderOut.shape, codebook_loss, commitment_loss, encoding_indices.shape, perplexity, diversity_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2b85092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 40, 1024])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Text2Video(nn.Module):\n",
    "    def __init__(self, embedDimension, sequenceLength, codeBookDim, hiddenLayers, heads, feedForwardDim, text_max_length=128, drop=0.15):\n",
    "        super().__init__()\n",
    "        self.max_length = text_max_length\n",
    "        self.embedDimension = embedDimension\n",
    "        self.codeBookDim = codeBookDim\n",
    "        self.sequenceLength = sequenceLength\n",
    "\n",
    "        self.berTokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        self.bertModel = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        \n",
    "        for param in self.bertModel.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.hiddenSize = self.bertModel.config.hidden_size\n",
    "        \n",
    "        self.textProjection = nn.Linear(self.hiddenSize, embedDimension)\n",
    "        self.positionalEmbedding = nn.Embedding(self.max_length, embedDimension)\n",
    "        self.temporalPositionalEmbedding = nn.Embedding(self.sequenceLength, embedDimension)\n",
    "\n",
    "        self.textMultiAttention = nn.MultiheadAttention(embedDimension, heads, dropout=drop, batch_first=True)\n",
    "        self.textlayerNorm = nn.LayerNorm(embedDimension)\n",
    "\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=embedDimension, \n",
    "            nhead=heads, \n",
    "            dim_feedforward=feedForwardDim, \n",
    "            dropout=drop, \n",
    "            batch_first=True,\n",
    "            activation='gelu'\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer=self.decoder_layer, num_layers=hiddenLayers)\n",
    "        self.decoder_norm = nn.LayerNorm(embedDimension)\n",
    "        self.predictIndices = nn.Linear(embedDimension, codeBookDim)\n",
    "\n",
    "    def forward(self, text, device):\n",
    "        if isinstance(text, str):\n",
    "            text = [text]\n",
    "        elif isinstance(text, (list, tuple)):\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(f\"Give string or list of strings, recieved this {type(text)}\")\n",
    "            \n",
    "        batchSize = len(text)\n",
    "\n",
    "        tokens = self.berTokenizer(text, return_tensors='pt', padding='max_length', \n",
    "                                  truncation=True, max_length=self.max_length).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.bertModel(**tokens)\n",
    "            lastLayerEMbeddings = outputs.last_hidden_state\n",
    "        \n",
    "        positions = torch.arange(0, self.max_length, device=lastLayerEMbeddings.device).unsqueeze(0).expand(batchSize, -1)\n",
    "        positionalEmbeddings = self.positionalEmbedding(positions)\n",
    "        \n",
    "        textEmbeddings = self.textProjection(lastLayerEMbeddings)\n",
    "        textEmbeddings = textEmbeddings + positionalEmbeddings\n",
    "        textEmbeddings = self.textlayerNorm(textEmbeddings)\n",
    "        \n",
    "        temporalPositions = torch.arange(0, self.sequenceLength, device=device).unsqueeze(0).expand(batchSize, -1)\n",
    "        temporal_queries = self.temporalPositionalEmbedding(temporalPositions)\n",
    "        \n",
    "        frame_text_features, _ = self.textMultiAttention(\n",
    "            query=temporal_queries,\n",
    "            key=textEmbeddings,\n",
    "            value=textEmbeddings\n",
    "        )\n",
    "        \n",
    "        causal_mask = torch.triu(torch.ones(self.sequenceLength, self.sequenceLength, device=device), diagonal=1).bool()\n",
    "        \n",
    "        decoderOut = self.decoder(\n",
    "            tgt=frame_text_features, \n",
    "            memory=textEmbeddings, \n",
    "            tgt_mask=causal_mask\n",
    "        )\n",
    "        decoderOut = self.decoder_norm(decoderOut)\n",
    "        \n",
    "        encoding_indices = self.predictIndices(decoderOut)\n",
    "        return encoding_indices\n",
    "    \n",
    "t2v = Text2Video(embedDimension=256, sequenceLength=40, codeBookDim=1024, hiddenLayers=6, heads=8, feedForwardDim=2048, drop=0.15)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "t2v.to(device)\n",
    "\n",
    "text = [\"a cat jumping on a bed\", \"A man Walking\", \"He is Running\"]\n",
    "logits = t2v(text, device)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a483f8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "embedDimension = 256\n",
    "sequenceLength = 40\n",
    "codeBookDim = 1024\n",
    "hiddenLayers=6\n",
    "heads=8\n",
    "feedForwardDim=2048\n",
    "drop=0.15\n",
    "learning_rate = 3e-4\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tranform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "torchDataset = FrameDataset(dataset, totalSequence=sequenceLength, transform=tranform)\n",
    "dataloader = DataLoader(torchDataset, batch_size=BATCH_SIZE, shuffle = True, num_workers=8, persistent_workers=True)\n",
    "model = Text2Video(embedDimension=embedDimension, sequenceLength=sequenceLength, codeBookDim=codeBookDim, hiddenLayers=hiddenLayers, heads=heads, feedForwardDim=feedForwardDim, drop=drop)\n",
    "model = torch.nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "lossFn =  nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate)#, weight_decay=1e-5)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dd7b9259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0/1000:   0%|          | 0/5000 [02:21<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     44\u001b[39m     loop.set_postfix({\n\u001b[32m     45\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTotalL\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecoderLoss\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     46\u001b[39m     })\n\u001b[32m     48\u001b[39m decoderLoss /= \u001b[38;5;28mlen\u001b[39m(dataloader)   \n\u001b[32m     50\u001b[39m torch.save({\n\u001b[32m     51\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m'\u001b[39m: each_epoch,\n\u001b[32m     52\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmodel_state_dict\u001b[39m\u001b[33m'\u001b[39m: modelA.module.state_dict(),\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     \u001b[33m'\u001b[39m\u001b[33moptimizer_state_dict\u001b[39m\u001b[33m'\u001b[39m: \u001b[43moptimizer\u001b[49m.state_dict(),\n\u001b[32m     54\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mscheduler_state_dict\u001b[39m\u001b[33m'\u001b[39m: scheduler.state_dict()\n\u001b[32m     55\u001b[39m }, checkpoint_path)\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# wandb.log({\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m#     \"Learning Rate\": optimizer.param_groups[0]['lr'],\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m#     \"Decoder Loss\": decoderLoss\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# })\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "\n",
    "checkpoint_path = \"/Users/ishananand/Desktop/Text-To-Video-Generation/videoModels/t2VModel.pt\"\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "    modelA.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    print(f\"Resuming from epoch {start_epoch}\")\n",
    "else:\n",
    "    print(\"Loading pretrained model...\")\n",
    "    # modelValA = torch.load(\"./projects/t2v-gif/models/t2VModel.pt\", map_location=torch.device('cpu'))\n",
    "    # modelA.load_state_dict(modelValA)\n",
    "\n",
    "modelA = torch.nn.DataParallel(modelA)\n",
    "modelA.to(device)\n",
    "\n",
    "for each_epoch in range(start_epoch, epochs):\n",
    "    modelA.train()\n",
    "    decoderLoss = 0.0\n",
    "    \n",
    "    loop = tqdm(dataloader, f\"{each_epoch}/{epochs}\")\n",
    "\n",
    "    for X, Y in loop:\n",
    "        # print(X.shape, Y)\n",
    "        with torch.no_grad():\n",
    "            _, _, _, _, encoding_indices, _, _ = modelA(X)\n",
    "        \n",
    "        y_pred = model(Y, device)\n",
    "        break\n",
    "        print(y_pred.shape, encoding_indices.shape)\n",
    "        y_pred_reshaed = rearrange(y_pred, 'b t d -> (b t) d')\n",
    "        encoding_indices_flat = rearrange(y_pred, 'b t d -> (b t) d', b = BATCH_SIZE, t = sequenceLength, d = codeBookdim)\n",
    "        loss = lossFn(y_pred_reshaed, encoding_indices_flat)\n",
    "        lossVal += loss.item()\n",
    "   \n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        loop.set_postfix({\n",
    "            \"TotalL\": f\"{decoderLoss}\"\n",
    "        })\n",
    "\n",
    "    decoderLoss /= len(dataloader)   \n",
    "    \n",
    "    torch.save({\n",
    "        'epoch': each_epoch,\n",
    "        'model_state_dict': modelA.module.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict()\n",
    "    }, checkpoint_path)\n",
    "    break\n",
    "    \n",
    "    # wandb.log({\n",
    "    #     \"Learning Rate\": optimizer.param_groups[0]['lr'],\n",
    "    #     \"Decoder Loss\": decoderLoss\n",
    "    # })\n",
    "    scheduler.step()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674acc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lossVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de48f53b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763c8369",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
