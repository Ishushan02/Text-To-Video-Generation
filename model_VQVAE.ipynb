{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b646838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchview import draw_graph\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional  as Fn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from io import BytesIO\n",
    "from IPython.core.display import HTML\n",
    "from IPython.display import Image as IPyImage, display\n",
    "from PIL import Image, ImageSequence\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from piq import ssim\n",
    "import gc\n",
    "import pandas as pd\n",
    "import urllib\n",
    "import cv2\n",
    "# import wandb\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dffb08ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(\n",
    "#     project=\"Transformer-Decoder\",  \n",
    "#     name=\"experiment-2\",    \n",
    "#     # id=\"uqcub7jq\",  \n",
    "#     # resume=\"allow\",\n",
    "#     # config={                       \n",
    "#     #     \"epochs\": 1000,\n",
    "#     #     \"batch_size\": 64,\n",
    "#     # }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "967b1300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1967,  0.2217,  0.1271,  ..., -0.2151, -0.2055,  0.1710],\n",
       "         [ 0.0246, -0.0845, -0.2174,  ..., -0.1461,  0.1783,  0.1972],\n",
       "         [-0.0138, -0.0256, -0.0489,  ..., -0.1537,  0.1355, -0.0149],\n",
       "         ...,\n",
       "         [-0.0949,  0.0910,  0.0144,  ..., -0.2327, -0.0309, -0.1330],\n",
       "         [-0.1372,  0.1921,  0.1300,  ..., -0.0541, -0.0568,  0.2247],\n",
       "         [-0.1316, -0.2259,  0.0334,  ..., -0.1651,  0.2379,  0.1185]],\n",
       "        grad_fn=<MmBackward0>),\n",
       " tensor([36, 52, 45,  ...,  5, 20,  4]),\n",
       " tensor(59.5750),\n",
       " tensor(0.0172))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorQuantizeImage(nn.Module):\n",
    "    def __init__(self, codeBookDim = 64, embeddingDim = 32, decay = 0.99, eps = 1e-5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.codeBookDim = codeBookDim\n",
    "        self.embeddingDim = embeddingDim\n",
    "        self.decay = decay\n",
    "        self.eps = eps\n",
    "        self.dead_codeBook_threshold = codeBookDim * 0.6\n",
    "\n",
    "        self.codebook = nn.Embedding(codeBookDim, embeddingDim)\n",
    "        nn.init.xavier_uniform_(self.codebook.weight.data)\n",
    "\n",
    "        self.register_buffer('ema_Count', torch.zeros(codeBookDim))\n",
    "        self.register_buffer('ema_Weight', self.codebook.weight.data.clone())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_reshaped = x.view(-1, self.embeddingDim)\n",
    "\n",
    "        distance = (torch.sum(x_reshaped**2, dim=1, keepdim=True) \n",
    "                    + torch.sum(self.codebook.weight**2, dim=1)\n",
    "                    - 2 * torch.matmul(x_reshaped, self.codebook.weight.t()))\n",
    "        \n",
    "        encoding_indices = torch.argmin(distance, dim=1) \n",
    "        encodings = Fn.one_hot(encoding_indices, self.codeBookDim).type(x_reshaped.dtype)\n",
    "        quantized = torch.matmul(encodings, self.codebook.weight)\n",
    "\n",
    "        if self.training:\n",
    "            self.ema_Count = self.decay * self.ema_Count + (1 - self.decay) * torch.sum(encodings, 0)\n",
    "            \n",
    "            x_reshaped_sum = torch.matmul(encodings.t(), x_reshaped.detach())\n",
    "            self.ema_Weight = self.decay * self.ema_Weight + (1 - self.decay) * x_reshaped_sum\n",
    "            \n",
    "            n = torch.clamp(self.ema_Count, min=self.eps)\n",
    "            updated_embeddings = self.ema_Weight / n.unsqueeze(1)\n",
    "            self.codebook.weight.data.copy_(updated_embeddings)\n",
    "\n",
    "        \n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        log_encoding_sum = -torch.sum(avg_probs * torch.log(avg_probs + 1e-10))\n",
    "        perplexity = torch.exp(log_encoding_sum)\n",
    "\n",
    "        entropy = log_encoding_sum\n",
    "        normalized_entropy = entropy / torch.log(torch.tensor(self.codeBookDim, device=x.device))\n",
    "        diversity_loss = 1.0 - normalized_entropy\n",
    "\n",
    "        return quantized, encoding_indices, perplexity, diversity_loss\n",
    "        \n",
    "        \n",
    "vq = VectorQuantizeImage(codeBookDim=64,embeddingDim=32)\n",
    "rand = torch.randn(1024,32)\n",
    "vq(rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f095d446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64, 2, 16, 16]),\n",
       " torch.Size([32, 3, 8, 64, 64]),\n",
       " tensor(0.1852, grad_fn=<MseLossBackward0>),\n",
       " tensor(0.1852, grad_fn=<MseLossBackward0>),\n",
       " torch.Size([32, 2, 16, 16]),\n",
       " tensor(41.6775),\n",
       " tensor(0.2313))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VecQVAE3D(nn.Module):\n",
    "    def __init__(self, inChannels=3, hiddenDim=64, codeBookdim=128, embedDim=64):\n",
    "        super().__init__()\n",
    "        self.inChannels = inChannels\n",
    "        self.hiddenDim = hiddenDim\n",
    "        self.codeBookdim = codeBookdim\n",
    "        self.embedDim = embedDim\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv3d(inChannels, hiddenDim, kernel_size=4, stride=2, padding=1),  # ↓ t,h,w\n",
    "            nn.BatchNorm3d(hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv3d(hiddenDim, hiddenDim, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm3d(hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv3d(hiddenDim, 2 * hiddenDim, kernel_size=4, stride=2, padding=1),  # ↓ t,h,w\n",
    "            nn.BatchNorm3d(2 * hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv3d(2 * hiddenDim, 2 * hiddenDim, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm3d(2 * hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv3d(2 * hiddenDim, embedDim, kernel_size=1)\n",
    "        )\n",
    "\n",
    "        self.vector_quantize = VectorQuantizeImage(codeBookDim=codeBookdim, embeddingDim=embedDim)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose3d(embedDim, 2 * hiddenDim, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm3d(2 * hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv3d(2 * hiddenDim, 2 * hiddenDim, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm3d(2 * hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose3d(2 * hiddenDim, hiddenDim, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm3d(hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv3d(hiddenDim, hiddenDim, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm3d(hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv3d(hiddenDim, inChannels, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encodeImage(self, x, noise_std=0.15):\n",
    "        encodedOut = self.encoder(x)\n",
    "        if self.training:\n",
    "            encodedOut = encodedOut + torch.randn_like(encodedOut) * noise_std\n",
    "        return encodedOut\n",
    "\n",
    "    def decodeImage(self, quantized_vector):\n",
    "        return self.decoder(quantized_vector)\n",
    "\n",
    "    def forward(self, x):  \n",
    "        B, T, C, H, W = x.shape\n",
    "        x = x.view(B, C, T, H, W)  \n",
    "\n",
    "        encodedOut = self.encodeImage(x)\n",
    "        B, D, T1, H1, W1 = encodedOut.shape\n",
    "\n",
    "        flat_feat = rearrange(encodedOut, 'b d t h w -> (b t h w) d')\n",
    "        quantized_vectors, encoding_indices, perplexity, diversity_loss = self.vector_quantize(flat_feat)\n",
    "\n",
    "        codebook_loss = Fn.mse_loss(flat_feat.detach(), quantized_vectors)\n",
    "        commitment_loss = Fn.mse_loss(flat_feat, quantized_vectors.detach())\n",
    "\n",
    "        quantized_vectors = flat_feat + (quantized_vectors - flat_feat).detach()\n",
    "        decoder_input = rearrange(quantized_vectors, '(b t h w) d -> b d t h w', b=B, t=T1, h=H1, w=W1)\n",
    "\n",
    "        decodedOut = self.decodeImage(decoder_input)\n",
    "        encoding_indices = encoding_indices.view(B, T1, H1, W1)\n",
    "\n",
    "        return decoder_input, decodedOut, codebook_loss, commitment_loss, encoding_indices, perplexity, diversity_loss\n",
    "\n",
    "\n",
    "VQ = VecQVAE3D(inChannels = 3, hiddenDim = 256, codeBookdim = 128, embedDim = 64)\n",
    "test = torch.randn(32, 10, 3, 64, 64)\n",
    "quantized_latents, decoderOut, codebook_loss, commitment_loss, encoding_indices, perplexity, diversity_loss = VQ(test)\n",
    "quantized_latents.shape, decoderOut.shape, codebook_loss, commitment_loss, encoding_indices.shape, perplexity, diversity_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3eff123c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1310720"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*10*64*64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51095fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 64, 16, 16]),\n",
       " torch.Size([32, 3, 64, 64]),\n",
       " tensor(0.1670, grad_fn=<MseLossBackward0>),\n",
       " tensor(0.1670, grad_fn=<MseLossBackward0>),\n",
       " torch.Size([8192]),\n",
       " tensor(48.2622),\n",
       " tensor(0.2010))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VecQVAE(nn.Module):\n",
    "    def __init__(self, inChannels = 1, hiddenDim = 32, codeBookdim = 128, embedDim = 128):\n",
    "        super().__init__()\n",
    "        self.inChannels = inChannels\n",
    "        self.hiddenDim = hiddenDim\n",
    "        self.codeBookdim = codeBookdim\n",
    "        self.embedDim = embedDim\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(inChannels, hiddenDim, 4, 2, 1), \n",
    "            nn.BatchNorm2d(hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(hiddenDim, hiddenDim, 3, 1, 1),\n",
    "            nn.BatchNorm2d(hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(hiddenDim, 2 * hiddenDim, 4, 2, 1),\n",
    "            nn.BatchNorm2d(2 * hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(2 * hiddenDim, 2 * hiddenDim, 3, 1, 1),\n",
    "            nn.BatchNorm2d(2 * hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(2 * hiddenDim, embedDim, 1),\n",
    "        )\n",
    "\n",
    "        self.vector_quantize = VectorQuantizeImage(codeBookDim=codeBookdim,embeddingDim=embedDim)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(embedDim, 2 * hiddenDim, 4, 2, 1),\n",
    "            nn.BatchNorm2d(2 * hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(2 * hiddenDim, 2 * hiddenDim, 3, 1, 1),\n",
    "            nn.BatchNorm2d(2 * hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(2 * hiddenDim, hiddenDim, 4, 2, 1),\n",
    "            nn.BatchNorm2d(hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(hiddenDim, hiddenDim, 3, 1, 1),\n",
    "            nn.BatchNorm2d(hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(hiddenDim, inChannels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encodeImage(self, x, noise_std = 0.15):\n",
    "        if self.training:\n",
    "            encodedOut = self.encoder(x)\n",
    "            encodedOut = encodedOut + torch.randn_like(encodedOut) * noise_std\n",
    "        else:\n",
    "            encodedOut = self.encoder(x)\n",
    "\n",
    "        return encodedOut\n",
    "\n",
    "    def decodeImage(self, quantized_vector):\n",
    "        decodedOut = self.decoder(quantized_vector)\n",
    "        return decodedOut\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, inChannels, height, width = x.shape\n",
    "        encodedOut = self.encodeImage(x)\n",
    "        batch_size, encoded_channel, encoded_height, encoded_width = encodedOut.shape\n",
    "        \n",
    "        # print(f\"Encoded Shape: {encodedOut.shape}\")\n",
    "\n",
    "        \n",
    "        vectorize_input = rearrange(encodedOut, 'b c h w -> (b h w) c')\n",
    "        quantized_vectors, encoding_indices, perplexity, diversity_loss  = self.vector_quantize(vectorize_input)\n",
    "        codebook_loss = Fn.mse_loss(vectorize_input.detach(), quantized_vectors)\n",
    "        commitment_loss = Fn.mse_loss(vectorize_input, quantized_vectors.detach())\n",
    "\n",
    "        quantized_vectors = vectorize_input + (quantized_vectors - vectorize_input).detach()\n",
    "        # print(f\"CodeBook Loss: {codebook_loss} , Commitment Loss: {commitment_loss}\")\n",
    "        # print(f\"Quantized SHape: {quantized_vectors.shape}\")\n",
    "\n",
    "        decoder_input = rearrange(quantized_vectors, '(b h w) d -> b d h w', d = encoded_channel, h = encoded_height, w = encoded_width)\n",
    "        # print(f\"Decoded Input SHape: {decoder_input.shape}\")\n",
    "        decodedOut = self.decodeImage(decoder_input)\n",
    "\n",
    "        # print(f\"Decoded SHape: {decodedOut.shape}\")\n",
    "        \n",
    "        return decoder_input, decodedOut, codebook_loss, commitment_loss, encoding_indices, perplexity, diversity_loss\n",
    "\n",
    "VQ = VecQVAE(inChannels = 3, hiddenDim = 256, codeBookdim = 128, embedDim = 64)\n",
    "test = torch.randn(32, 3, 64, 64)\n",
    "quantized_latents, decoderOut, codebook_loss, commitment_loss, encoding_indices, perplexity, diversity_loss = VQ(test)\n",
    "quantized_latents.shape, decoderOut.shape, codebook_loss, commitment_loss, encoding_indices.shape, perplexity, diversity_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f7860d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78568, 3)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"/Users/ishananand/Desktop/Text-To-Video-Generation/data/modified_tgif.csv\")\n",
    "dataset = dataset[(dataset['frames'] <= 40) & (dataset['frames'] > 15)].copy().reset_index(drop=True)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6aa29ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(377, 500, 3)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getNumpyArray(dataset, index):\n",
    "    url = dataset['url'][index]\n",
    "    resp = urllib.request.urlopen(url)\n",
    "    image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
    "    image = cv2.imdecode(image, cv2.IMREAD_COLOR_RGB)\n",
    "\n",
    "    return image\n",
    "\n",
    "tImg = getNumpyArray(dataset, 10)\n",
    "tImg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4430799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 500, 500, 3)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "def getNumpyArray(dataset, index):\n",
    "    url = dataset.iloc[index]['url']\n",
    "    resp = urllib.request.urlopen(url)\n",
    "    data = resp.read()\n",
    "    pil_image = Image.open(io.BytesIO(data))\n",
    "    \n",
    "    frames = []\n",
    "    for frame in ImageSequence.Iterator(pil_image):\n",
    "        frame = frame.convert(\"RGB\")\n",
    "        frame_np = np.array(frame)\n",
    "        frames.append(frame_np)\n",
    "    \n",
    "    frames = np.array(frames)\n",
    "    return frames\n",
    "\n",
    "\n",
    "tImg = getNumpyArray(dataset, 74801)\n",
    "tImg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1cecd366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a man is shaking a dog's hand. w\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://33.media.tumblr.com/b2b7aa8727c11a72e3ac761e80d8f19d/tumblr_nqlx8zU9yJ1uzvurno1_250.gif\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset['caption'][2222])\n",
    "HTML(f'<img src=\"{dataset['url'][2222]}\" />')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "060176de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 3, 128, 128]) a guy holds a drink and looks up\n"
     ]
    }
   ],
   "source": [
    "class FrameDataset(Dataset):\n",
    "    def __init__(self, data, totalSequence = 40, transform = None):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def npArray(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        url = row['url']\n",
    "        resp = urllib.request.urlopen(url)\n",
    "        # image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
    "        # image = cv2.imdecode(image, cv2.IMREAD_COLOR_RGB)\n",
    "        # return image\n",
    "        image_data = resp.read()\n",
    "        img = Image.open(io.BytesIO(image_data))\n",
    "\n",
    "        frames = []\n",
    "        for frame in ImageSequence.Iterator(img):\n",
    "            frame_rgb = frame.convert(\"RGB\")\n",
    "            frames.append(np.array(frame_rgb))\n",
    "\n",
    "        return frames\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # print(index)\n",
    "        gif = self.npArray(index)\n",
    "        caption = self.data.iloc[index]['caption']\n",
    "        totalframes = self.data.iloc[index]['frames']\n",
    "        \n",
    "        if totalframes < 40 :\n",
    "            last_frame = gif[-1:]\n",
    "            padded_frames = np.repeat(last_frame, 40 - totalframes, axis=0)\n",
    "            gif = np.concatenate((gif, padded_frames), axis=0)\n",
    "\n",
    "        tensorFrames = []\n",
    "        for frame in gif:\n",
    "            pil_frame = Image.fromarray(frame)\n",
    "            tensor_frame = self.transform(pil_frame)\n",
    "            tensorFrames.append(tensor_frame)\n",
    "\n",
    "\n",
    "        tensorFrames = torch.stack(tensorFrames)\n",
    "        tensorFrames = tensorFrames/255.0\n",
    "\n",
    "        return tensorFrames, caption\n",
    "    \n",
    "\n",
    "tranform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "fdata = FrameDataset(dataset, transform=tranform)\n",
    "\n",
    "X, Y = fdata.__getitem__(74801)\n",
    "print(X.shape, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9bafbc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "codeBookdim = 128\n",
    "embedDim = 64\n",
    "hiddenDim = 256\n",
    "inChannels = 3\n",
    "tranform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "torchDataset = FrameDataset(dataset, transform=tranform)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dataloader = DataLoader(torchDataset, batch_size=BATCH_SIZE, shuffle = True)\n",
    "modelA = VecQVAE(inChannels = inChannels, hiddenDim = hiddenDim, codeBookdim = codeBookdim, embedDim = embedDim).to(device)\n",
    "lossFn = nn.MSELoss()\n",
    "optimizerA = torch.optim.Adam([\n",
    "                    {'params': modelA.encoder.parameters(), 'lr': 2e-4},\n",
    "                    {'params': modelA.decoder.parameters(), 'lr': 2e-4},\n",
    "                    {'params': modelA.vector_quantize.parameters(), 'lr': 1e-4}\n",
    "                ], weight_decay=1e-5)\n",
    "schedulerA = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                optimizerA, T_0=10, T_mult=2, eta_min=1e-6\n",
    "            )\n",
    "\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "76ed20f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.Size([64, 40, 3, 128, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48031bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0/1000:   0%|          | 0/1228 [00:39<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "a Tensor with 64 elements cannot be converted to Scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[133]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m X, caption, n \u001b[38;5;129;01min\u001b[39;00m loop:\n\u001b[32m     17\u001b[39m     X = X.to(device)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[32m     19\u001b[39m         \u001b[38;5;28mprint\u001b[39m(X[:,i].shape)\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# Y = Y.to(device)\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: a Tensor with 64 elements cannot be converted to Scalar"
     ]
    }
   ],
   "source": [
    "# modelValA = torch.load(\"./model/VQVAE/vqvae.pt\", map_location=torch.device('cpu'))\n",
    "# modelA.load_state_dict(modelValA)\n",
    "\n",
    "for each_epoch in range(epochs):\n",
    "    modelA.train()\n",
    "    reconstruct_loss = 0.0\n",
    "    codeb_loss = 0.0\n",
    "    commit_loss = 0.0\n",
    "    vqvaeloss = 0.0\n",
    "    diverse_loss = 0.0\n",
    "    ssim_loss = 0.0\n",
    "    \n",
    "    loop = tqdm(dataloader, f\"{each_epoch}/{epochs}\")\n",
    "    perplexities = []\n",
    "\n",
    "    for X, _, n in loop:\n",
    "        X = X.to(device)\n",
    "        # Y = Y.to(device)\n",
    "        \n",
    "        quantized_latents, decoderOut, codebook_loss, commitment_loss, encoding_indices, perplexity, diversity_loss = modelA(X)\n",
    "\n",
    "        ssim_score = ssim(X, decoderOut, data_range=1.0)\n",
    "        ssim_loss = 1.0 - ssim_score\n",
    "\n",
    "        reconstruction_loss = torch.mean((X - decoderOut)**2)\n",
    "        \n",
    "        loss = reconstruction_loss + codebook_loss + 0.2 * commitment_loss + 0.1 * diversity_loss + 0.1 * ssim_loss\n",
    "        vqvaeloss += loss.item()\n",
    "\n",
    "        \n",
    "        reconstruct_loss += reconstruction_loss.item()\n",
    "        diverse_loss += diversity_loss.item()\n",
    "        codeb_loss += codebook_loss.item()\n",
    "        commit_loss += commitment_loss.item()\n",
    "        ssim_loss += ssim_loss.item()\n",
    "        perplexities.append(perplexity)\n",
    "        \n",
    "        \n",
    "        optimizerA.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(modelA.parameters(), max_norm=1.0)\n",
    "        optimizerA.step()\n",
    "        loop.set_postfix({\"TotalL\": f\"{vqvaeloss}\", \"ReconsL\": f\"{reconstruct_loss}\", \"CodeL\":f\"{codeb_loss}\",\n",
    "                          \"CommitL\":f\"{commitment_loss}\", \"Perplexity\":f\"{perplexity}\", \"Diversity Loss\":f\"{diverse_loss}\", \"SSIM Loss\":f\"{ssim_loss}\"})\n",
    "    #     break\n",
    "    # break\n",
    "\n",
    "    average_perplexity = sum(perplexities)/len(perplexities)\n",
    "    vqvaeloss /= len(dataloader)   \n",
    "    reconstruct_loss /= len(dataloader)   \n",
    "    codeb_loss /= len(dataloader)   \n",
    "    commit_loss /= len(dataloader)   \n",
    "    diverse_loss /= len(dataloader)\n",
    "    torch.save(modelA.state_dict(), \"./model/VQVAE/vqvae.pt\")\n",
    "    wandb.log({\n",
    "        \"Epoch\": each_epoch,\n",
    "        \"VQVAE LR\": optimizerA.param_groups[0]['lr'],\n",
    "        \"VQVAE Loss\": vqvaeloss,\n",
    "        \"Reconstruction Loss\": reconstruct_loss,\n",
    "        \"Codebook Loss\": codeb_loss,\n",
    "        \"Commitment Loss\": commit_loss,\n",
    "        \"Diversity Loss\": diverse_loss,\n",
    "        \"Perplexity\": average_perplexity,\n",
    "        \"SSIM Loss\":ssim_loss,\n",
    "    })\n",
    "    schedulerA.step()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afbfe8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
