{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b646838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchview import draw_graph\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional  as Fn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from io import BytesIO\n",
    "from IPython.core.display import HTML\n",
    "from IPython.display import Image as IPyImage, display\n",
    "from PIL import Image, ImageSequence\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from piq import ssim\n",
    "import gc\n",
    "import pandas as pd\n",
    "import urllib\n",
    "from torchvision.models import vgg16\n",
    "import io\n",
    "import cv2\n",
    "import hashlib\n",
    "import kornia\n",
    "\n",
    "# import wandb\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dffb08ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(\n",
    "#     project=\"Transformer-Decoder\",  \n",
    "#     name=\"experiment-2\",    \n",
    "#     # id=\"uqcub7jq\",  \n",
    "#     # resume=\"allow\",\n",
    "#     # config={                       \n",
    "#     #     \"epochs\": 1000,\n",
    "#     #     \"batch_size\": 64,\n",
    "#     # }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "967b1300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.7902e-04, -2.3358e-01, -1.5296e-01,  ...,  1.9598e-01,\n",
       "           3.0143e-03,  1.8851e-01],\n",
       "         [-5.2693e-02,  2.1346e-01,  1.5475e-01,  ..., -2.4188e-01,\n",
       "          -1.2677e-01, -1.3929e-01],\n",
       "         [-1.5904e-02, -5.5351e-02, -9.2306e-02,  ...,  1.5144e-02,\n",
       "           1.9452e-01, -1.5721e-01],\n",
       "         ...,\n",
       "         [ 8.6697e-02,  1.9576e-01, -7.8892e-02,  ...,  7.5265e-02,\n",
       "           7.6978e-02,  2.1213e-01],\n",
       "         [-2.3134e-01,  1.2208e-01, -4.3722e-02,  ..., -5.7895e-03,\n",
       "           4.9764e-02, -8.8457e-03],\n",
       "         [-1.6841e-01, -6.7257e-02,  1.6089e-01,  ...,  2.0296e-01,\n",
       "          -7.7687e-02,  2.5948e-02]], grad_fn=<MmBackward0>),\n",
       " tensor([62, 51, 45,  ...,  6, 15, 61]),\n",
       " tensor(59.8050),\n",
       " tensor(0.0163))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorQuantizeImage(nn.Module):\n",
    "    def __init__(self, codeBookDim = 64, embeddingDim = 32, decay = 0.99, eps = 1e-5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.codeBookDim = codeBookDim\n",
    "        self.embeddingDim = embeddingDim\n",
    "        self.decay = decay\n",
    "        self.eps = eps\n",
    "        self.dead_codeBook_threshold = codeBookDim * 0.6\n",
    "\n",
    "        self.codebook = nn.Embedding(codeBookDim, embeddingDim)\n",
    "        nn.init.xavier_uniform_(self.codebook.weight.data)\n",
    "\n",
    "        self.register_buffer('ema_Count', torch.zeros(codeBookDim))\n",
    "        self.register_buffer('ema_Weight', self.codebook.weight.data.clone())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_reshaped = x.view(-1, self.embeddingDim)\n",
    "\n",
    "        distance = (torch.sum(x_reshaped**2, dim=1, keepdim=True) \n",
    "                    + torch.sum(self.codebook.weight**2, dim=1)\n",
    "                    - 2 * torch.matmul(x_reshaped, self.codebook.weight.t()))\n",
    "        \n",
    "        encoding_indices = torch.argmin(distance, dim=1) \n",
    "        encodings = Fn.one_hot(encoding_indices, self.codeBookDim).type(x_reshaped.dtype)\n",
    "        quantized = torch.matmul(encodings, self.codebook.weight)\n",
    "\n",
    "        if self.training:\n",
    "            self.ema_Count = self.decay * self.ema_Count + (1 - self.decay) * torch.sum(encodings, 0)\n",
    "            \n",
    "            x_reshaped_sum = torch.matmul(encodings.t(), x_reshaped.detach())\n",
    "            self.ema_Weight = self.decay * self.ema_Weight + (1 - self.decay) * x_reshaped_sum\n",
    "            \n",
    "            n = torch.clamp(self.ema_Count, min=self.eps)\n",
    "            updated_embeddings = self.ema_Weight / n.unsqueeze(1)\n",
    "            self.codebook.weight.data.copy_(updated_embeddings)\n",
    "\n",
    "        \n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        log_encoding_sum = -torch.sum(avg_probs * torch.log(avg_probs + 1e-10))\n",
    "        perplexity = torch.exp(log_encoding_sum)\n",
    "\n",
    "        entropy = log_encoding_sum\n",
    "        normalized_entropy = entropy / torch.log(torch.tensor(self.codeBookDim, device=x.device))\n",
    "        diversity_loss = 1.0 - normalized_entropy\n",
    "\n",
    "        return quantized, encoding_indices, perplexity, diversity_loss\n",
    "        \n",
    "        \n",
    "vq = VectorQuantizeImage(codeBookDim=64,embeddingDim=32)\n",
    "rand = torch.randn(1024,32)\n",
    "vq(rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eff123c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81920"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*10*256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51095fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([320, 64, 16, 16]),\n",
       " torch.Size([320, 3, 64, 64]),\n",
       " tensor(0.1676, grad_fn=<MseLossBackward0>),\n",
       " tensor(0.1676, grad_fn=<MseLossBackward0>),\n",
       " torch.Size([81920]),\n",
       " tensor(52.9498),\n",
       " tensor(0.1819))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VecQVAE(nn.Module):\n",
    "    def __init__(self, inChannels = 1, hiddenDim = 32, codeBookdim = 128, embedDim = 128):\n",
    "        super().__init__()\n",
    "        self.inChannels = inChannels\n",
    "        self.hiddenDim = hiddenDim\n",
    "        self.codeBookdim = codeBookdim\n",
    "        self.embedDim = embedDim\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(inChannels, hiddenDim, 4, 2, 1), \n",
    "            nn.BatchNorm2d(hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(hiddenDim, hiddenDim, 3, 1, 1),\n",
    "            nn.BatchNorm2d(hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(hiddenDim, 2 * hiddenDim, 4, 2, 1),\n",
    "            nn.BatchNorm2d(2 * hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(2 * hiddenDim, 2 * hiddenDim, 3, 1, 1),\n",
    "            nn.BatchNorm2d(2 * hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(2 * hiddenDim, embedDim, 1),\n",
    "        )\n",
    "\n",
    "        self.vector_quantize = VectorQuantizeImage(codeBookDim=codeBookdim,embeddingDim=embedDim)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(embedDim, 2 * hiddenDim, 4, 2, 1),\n",
    "            nn.BatchNorm2d(2 * hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(2 * hiddenDim, 2 * hiddenDim, 3, 1, 1),\n",
    "            nn.BatchNorm2d(2 * hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(2 * hiddenDim, hiddenDim, 4, 2, 1),\n",
    "            nn.BatchNorm2d(hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(hiddenDim, hiddenDim, 3, 1, 1),\n",
    "            nn.BatchNorm2d(hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(hiddenDim, inChannels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encodeImage(self, x, noise_std = 0.15):\n",
    "        if self.training:\n",
    "            encodedOut = self.encoder(x)\n",
    "            encodedOut = encodedOut + torch.randn_like(encodedOut) * noise_std\n",
    "        else:\n",
    "            encodedOut = self.encoder(x)\n",
    "\n",
    "        return encodedOut\n",
    "\n",
    "    def decodeImage(self, quantized_vector):\n",
    "        decodedOut = self.decoder(quantized_vector)\n",
    "        return decodedOut\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, time_frame, inChannels, height, width = x.shape\n",
    "\n",
    "        x_frames = rearrange(x, 'b t c h w -> (b t) c h w')\n",
    "        encodedOut = self.encodeImage(x_frames)\n",
    "        batch_size_time_frame, encoded_channel, encoded_height, encoded_width = encodedOut.shape\n",
    "        \n",
    "        # print(f\"Encoded Shape: {encodedOut.shape}\")\n",
    "\n",
    "        \n",
    "        vectorize_input = rearrange(encodedOut, 'bt d h w -> (bt h w) d')\n",
    "        quantized_vectors, encoding_indices, perplexity, diversity_loss  = self.vector_quantize(vectorize_input)\n",
    "        codebook_loss = Fn.mse_loss(vectorize_input.detach(), quantized_vectors)\n",
    "        commitment_loss = Fn.mse_loss(vectorize_input, quantized_vectors.detach())\n",
    "\n",
    "        quantized_vectors = vectorize_input + (quantized_vectors - vectorize_input).detach()\n",
    "        # print(f\"CodeBook Loss: {codebook_loss} , Commitment Loss: {commitment_loss}\")\n",
    "        # print(f\"Quantized SHape: {quantized_vectors.shape}\")\n",
    "\n",
    "        decoder_input = rearrange(quantized_vectors, '(bt h w) d -> bt d h w', bt = batch_size_time_frame, d = encoded_channel, h = encoded_height, w = encoded_width)\n",
    "        # print(f\"Decoded Input SHape: {decoder_input.shape}\")\n",
    "        decodedOut = self.decodeImage(decoder_input)\n",
    "\n",
    "        # print(f\"Decoded SHape: {decodedOut.shape}\")\n",
    "        \n",
    "        return decoder_input, decodedOut, codebook_loss, commitment_loss, encoding_indices, perplexity, diversity_loss\n",
    "\n",
    "VQ = VecQVAE(inChannels = 3, hiddenDim = 256, codeBookdim = 128, embedDim = 64)\n",
    "test = torch.randn(32, 10, 3, 64, 64)\n",
    "quantized_latents, decoderOut, codebook_loss, commitment_loss, encoding_indices, perplexity, diversity_loss = VQ(test)\n",
    "quantized_latents.shape, decoderOut.shape, codebook_loss, commitment_loss, encoding_indices.shape, perplexity, diversity_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2d40a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 128, 64, 64])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SelfAttentionBlock(nn.Module):\n",
    "    def __init__(self, inChannels, heads = 8):\n",
    "        super().__init__()\n",
    "        self.query = nn.Conv2d(inChannels, inChannels // heads, 1)\n",
    "        self.key = nn.Conv2d(inChannels, inChannels // heads, 1)\n",
    "        self.value = nn.Conv2d(inChannels, inChannels, 1)\n",
    "\n",
    "        self.coeff = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, channel, height, width = x.shape\n",
    "        q = self.query(x).view(batch, -1, height * width).permute(0, 2, 1)\n",
    "        k = self.key(x).view(batch, -1, height * width)\n",
    "        v = self.value(x).view(batch, -1, height * width)\n",
    "        attn = torch.matmul(q, k)                                          \n",
    "        attn = Fn.softmax(attn, dim=-1)\n",
    "        attn_reshaped = attn.permute(0, 2, 1) \n",
    "        \n",
    "        out = torch.matmul(v, attn_reshaped)                        \n",
    "        out = out.view(batch, channel, height, width)                    \n",
    "        out = self.coeff * out + x                                      \n",
    "        return out\n",
    "\n",
    "rad = torch.randn(10, 128, 64, 64)\n",
    "sAtt = SelfAttentionBlock(128, 4)\n",
    "out = sAtt(rad)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8eeb6967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 128, 16, 16]),\n",
       " torch.Size([10, 3, 64, 64]),\n",
       " tensor(0.1703, grad_fn=<MseLossBackward0>),\n",
       " tensor(0.1703, grad_fn=<MseLossBackward0>),\n",
       " torch.Size([2560]),\n",
       " tensor(259.9273),\n",
       " tensor(0.1978))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VecQVAE(nn.Module):\n",
    "    def __init__(self, inChannels=3, hiddenDim=256, codeBookdim=256, embedDim=128):\n",
    "        super().__init__()\n",
    "        self.inChannels = inChannels\n",
    "        self.hiddenDim = hiddenDim\n",
    "        self.codeBookdim = codeBookdim\n",
    "        self.embedDim = embedDim\n",
    "\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(inChannels, hiddenDim, 4, 2, 1),\n",
    "            nn.BatchNorm2d(hiddenDim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(hiddenDim, hiddenDim, 3, 1, 1),\n",
    "            nn.BatchNorm2d(hiddenDim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(hiddenDim, 2 * hiddenDim, 4, 2, 1),\n",
    "            nn.BatchNorm2d(2 * hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            SelfAttentionBlock(2 * hiddenDim)\n",
    "        )\n",
    "\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv2d(2 * hiddenDim, 2 * hiddenDim, 3, 1, 1),\n",
    "            nn.BatchNorm2d(2 * hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            SelfAttentionBlock(2 * hiddenDim)\n",
    "        )\n",
    "\n",
    "        self.block5 = nn.Sequential(\n",
    "            nn.Conv2d(2 * hiddenDim, embedDim, 1)\n",
    "        )\n",
    "\n",
    "        self.vector_quantize = VectorQuantizeImage(codeBookDim=codeBookdim, embeddingDim=embedDim)\n",
    "\n",
    "        self.block6 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(embedDim, 2 * hiddenDim, 1),\n",
    "            nn.BatchNorm2d(2 * hiddenDim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.block7 = nn.Sequential(\n",
    "            nn.Conv2d(2 * hiddenDim, 2 * hiddenDim, 3, 1, 1),\n",
    "            nn.BatchNorm2d(2 * hiddenDim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.block8 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2 * hiddenDim, hiddenDim, 4, 2, 1),\n",
    "            nn.BatchNorm2d(hiddenDim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.block9 = nn.Sequential(\n",
    "            nn.Conv2d(hiddenDim, hiddenDim, 3, 1, 1),\n",
    "            nn.BatchNorm2d(hiddenDim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.block10 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(hiddenDim, hiddenDim // 2, 4, 2, 1),\n",
    "            nn.BatchNorm2d(hiddenDim // 2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "       \n",
    "        self.outputlayer = nn.Sequential(\n",
    "            nn.Conv2d(hiddenDim // 2, inChannels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encodeImage(self, x, noise_std=0.15):\n",
    "        if self.training:\n",
    "            x1 = self.block1(x)\n",
    "            x2 = self.block2(x1)\n",
    "            x3 = self.block3(x2)\n",
    "            x4 = self.block4(x3)\n",
    "            encoded = self.block5(x4)\n",
    "            encoded += torch.randn_like(encoded) * noise_std\n",
    "        else:\n",
    "            x1 = self.block1(x)\n",
    "            x2 = self.block2(x1)\n",
    "            x3 = self.block3(x2)\n",
    "            x4 = self.block4(x3)\n",
    "            encoded = self.block5(x4)\n",
    "        return encoded, (x2, x3, x4)\n",
    "\n",
    "    def decodeImage(self, quantized_vector, skips):\n",
    "        x2, x3, x4 = skips\n",
    "        # print(x2.shape, x3.shape, x4.shape)\n",
    "        x = self.block6(quantized_vector)\n",
    "        x = self.block7(x + x4)\n",
    "        x = self.block8(x + x3)\n",
    "        x = self.block9(x + x2)\n",
    "        x = self.block10(x)\n",
    "        return self.outputlayer(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, timeFrames, channel, height, width = x.shape\n",
    "        x = rearrange(x, 'b t c h w -> (b t) c h w')\n",
    "\n",
    "        encoded, skips = self.encodeImage(x)\n",
    "        batchTime, encodedChannels, encodedHeight, encodedWidth = encoded.shape\n",
    "        encoder_reshaped = rearrange(encoded, 'bt d h w -> (bt h w) d')\n",
    "        quantized_vectors, encoding_indices, perplexity, diversity_loss = self.vector_quantize(encoder_reshaped)\n",
    "\n",
    "        codebook_loss = Fn.mse_loss(encoder_reshaped.detach(), quantized_vectors)\n",
    "        commitment_loss = Fn.mse_loss(encoder_reshaped, quantized_vectors.detach())\n",
    "\n",
    "        quantized_vectors = encoder_reshaped + (quantized_vectors - encoder_reshaped).detach()\n",
    "        # print(quantized.shape)\n",
    "        decoder_input = rearrange(quantized_vectors, '(bt h w) d -> bt d h w', bt=batch * timeFrames, d=encodedChannels, h=encodedHeight, w=encodedWidth)\n",
    "        # print(decoder_input.shape)\n",
    "\n",
    "        decodedOut= self.decodeImage(decoder_input, skips)\n",
    "        return decoder_input, decodedOut, codebook_loss, commitment_loss, encoding_indices, perplexity, diversity_loss\n",
    "\n",
    "VQ = VecQVAE(inChannels = 3, hiddenDim = 512, codeBookdim = 1024, embedDim = 128)\n",
    "test = torch.randn(1, 10, 3, 64, 64)\n",
    "quantized_latents, decoderOut, codebook_loss, commitment_loss, encoding_indices, perplexity, diversity_loss = VQ(test)\n",
    "quantized_latents.shape, decoderOut.shape, codebook_loss, commitment_loss, encoding_indices.shape, perplexity, diversity_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f7860d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"/Users/ishananand/Desktop/Text-To-Video-Generation/data/modified_tgif.csv\")\n",
    "dataset = dataset[(dataset['frames'] <= 40) & (dataset['frames'] > 15)].copy().reset_index(drop=True)\n",
    "dataset = dataset[:10000] # thread 1 first 10000\n",
    "\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6aa29ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(377, 500, 3)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getNumpyArray(dataset, index):\n",
    "    url = dataset['url'][index]\n",
    "    resp = urllib.request.urlopen(url)\n",
    "    image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
    "    image = cv2.imdecode(image, cv2.IMREAD_COLOR_RGB)\n",
    "\n",
    "    return image\n",
    "\n",
    "tImg = getNumpyArray(dataset, 10)\n",
    "tImg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d4430799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 281, 500, 3)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getNumpyArray(dataset, index):\n",
    "    url = dataset.iloc[index]['url']\n",
    "    resp = urllib.request.urlopen(url)\n",
    "    data = resp.read()\n",
    "    pil_image = Image.open(io.BytesIO(data))\n",
    "    \n",
    "    frames = []\n",
    "    for frame in ImageSequence.Iterator(pil_image):\n",
    "        frame = frame.convert(\"RGB\")\n",
    "        frame_np = np.array(frame)\n",
    "        frames.append(frame_np)\n",
    "    \n",
    "    frames = np.array(frames)\n",
    "    return frames\n",
    "\n",
    "\n",
    "tImg = getNumpyArray(dataset, 2000)\n",
    "tImg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1cecd366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a man is shaking a dog's hand. w\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://33.media.tumblr.com/b2b7aa8727c11a72e3ac761e80d8f19d/tumblr_nqlx8zU9yJ1uzvurno1_250.gif\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset['caption'][2222])\n",
    "HTML(f'<img src=\"{dataset['url'][2222]}\" />')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "060176de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 3, 256, 256]) a man dressed in red is dancing.\n"
     ]
    }
   ],
   "source": [
    "class FrameDataset(Dataset):\n",
    "    def __init__(self, data, totalSequence = 40, transform = None):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.totalSequence = totalSequence\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def npArray(self, index):\n",
    "        try:\n",
    "            row = self.data.iloc[index]\n",
    "            totalframes = self.data.iloc[index]['frames']\n",
    "            url = row['url']\n",
    "            resp = urllib.request.urlopen(url)\n",
    "            image_data = resp.read()\n",
    "            img = Image.open(io.BytesIO(image_data))\n",
    "    \n",
    "            frames = []\n",
    "            for frame in ImageSequence.Iterator(img):\n",
    "                frame_rgb = frame.convert(\"RGB\")\n",
    "                frames.append(np.array(frame_rgb))\n",
    "    \n",
    "            return frames\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing index {index}: {e}\")\n",
    "            fallback = torch.zeros((256, 256, 3), dtype=torch.uint8)\n",
    "            return [fallback.numpy()]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # print(index)\n",
    "        gif = self.npArray(index)\n",
    "        caption = self.data.iloc[index]['caption']\n",
    "        totalframes = len(gif)#self.data.iloc[index]['frames']\n",
    "        \n",
    "        if totalframes < self.totalSequence:\n",
    "            gif += [gif[-1]] * (self.totalSequence - totalframes)\n",
    "\n",
    "        tensorFrames = torch.stack([\n",
    "            self.transform(Image.fromarray(frame)) for frame in gif\n",
    "        ])\n",
    "\n",
    "        tensorFrames = tensorFrames/255.0\n",
    "\n",
    "        return tensorFrames, caption\n",
    "    \n",
    "\n",
    "tranform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "fdata = FrameDataset(dataset, transform=tranform)\n",
    "\n",
    "X, Y = fdata.__getitem__(1)\n",
    "print(X.shape, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "66520a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 3, 256, 256]) man stood next to window looks to the side\n"
     ]
    }
   ],
   "source": [
    "class FrameDataset(Dataset):\n",
    "    def __init__(self, data, totalSequence=40, transform=None, cache_dir='/Users/ishananand/Desktop/Text-To-Video-Generation/data/cacheGIF'):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.totalSequence = totalSequence\n",
    "        self.cache_dir = cache_dir\n",
    "        os.makedirs(self.cache_dir, exist_ok=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def cache_path(self, index):\n",
    "        url = self.data.iloc[index]['url']\n",
    "        hash_name = hashlib.md5(url.encode()).hexdigest()\n",
    "        return os.path.join(self.cache_dir, f'{hash_name}.pt')\n",
    "\n",
    "    def npArray(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        url = row['url']\n",
    "        resp = urllib.request.urlopen(url)\n",
    "        image_data = resp.read()\n",
    "        img = Image.open(io.BytesIO(image_data))\n",
    "\n",
    "        frames = []\n",
    "        for frame in ImageSequence.Iterator(img):\n",
    "            frame_rgb = frame.convert(\"RGB\")\n",
    "            frames.append(np.array(frame_rgb))\n",
    "        return frames\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        cache_path = self.cache_path(index)\n",
    "\n",
    "        if os.path.exists(cache_path):\n",
    "            tensorFrames, caption = torch.load(cache_path)\n",
    "            return tensorFrames, caption\n",
    "\n",
    "        gif = self.npArray(index)\n",
    "        caption = self.data.iloc[index]['caption']\n",
    "        totalframes = self.data.iloc[index]['frames']\n",
    "\n",
    "        if totalframes < self.totalSequence:\n",
    "            gif += [gif[-1]] * (self.totalSequence - totalframes)\n",
    "        \n",
    "\n",
    "        tensorFrames = torch.stack([\n",
    "            self.transform(Image.fromarray(frame)) for frame in gif\n",
    "        ])\n",
    "\n",
    "        tensorFrames = tensorFrames / 255.0\n",
    "        torch.save((tensorFrames, caption), cache_path)\n",
    "\n",
    "        return tensorFrames, caption\n",
    "    \n",
    "tranform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "fdata = FrameDataset(dataset, transform=tranform)\n",
    "\n",
    "X, Y = fdata.__getitem__(2000)\n",
    "print(X.shape, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bafbc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "codeBookdim = 1024\n",
    "embedDim = 256\n",
    "hiddenDim = 512\n",
    "inChannels = 3\n",
    "tranform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "torchDataset = FrameDataset(dataset, transform=tranform)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dataloader = DataLoader(torchDataset, batch_size=BATCH_SIZE, shuffle = True)\n",
    "modelA = VecQVAE(inChannels = inChannels, hiddenDim = hiddenDim, codeBookdim = codeBookdim, embedDim = embedDim).to(device)\n",
    "lossFn = nn.MSELoss()\n",
    "optimizerA = torch.optim.Adam(\n",
    "                [\n",
    "                    {'params': modelA.parameters(), 'lr': 2e-4},\n",
    "                    # {'params': modelA.decodeImage.parameters(), 'lr': 2e-4},\n",
    "                    # {'params': modelA.vector_quantize.parameters(), 'lr': 1e-4}\n",
    "                ], weight_decay=1e-5)\n",
    "schedulerA = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                optimizerA, T_0=10, T_mult=2, eta_min=1e-6\n",
    "            )\n",
    "\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d743081",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ishananand/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/ishananand/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31.302576065063477"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def perceptualLoss(pred, target):\n",
    "    vgg = vgg16(pretrained = True).features[:17].eval()\n",
    "    vgg.to(device)\n",
    "    for param in vgg.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # print(pred.shape)\n",
    "    batch, channels, height, width = pred.shape\n",
    "\n",
    "    pred = pred.view(batch, channels, height, width)\n",
    "    target = target.view(batch, channels, height, width)\n",
    "\n",
    "    if pred.shape[1] == 1:\n",
    "        pred = pred.repeat(1, 3, 1, 1)\n",
    "        target = target.repeat(1, 3, 1, 1)\n",
    "\n",
    "\n",
    "    vgg_pred = vgg(pred).to(device)\n",
    "    vgg_true = vgg(target).to(device)\n",
    "\n",
    "    perceptualoss = Fn.mse_loss(vgg_pred, vgg_true)\n",
    "    return perceptualoss\n",
    "\n",
    "pred = torch.randn(1, 10, 3, 10, 10)\n",
    "pred1 = torch.randn(1, 10, 3, 10, 10)\n",
    "out = perceptualLoss(pred, pred1)\n",
    "out.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "76ed20f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19654.966796875"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lab_color_loss(pred, target):\n",
    "    pred_lab = kornia.color.rgb_to_lab(pred)\n",
    "    target_lab = kornia.color.rgb_to_lab(target)\n",
    "    loss = Fn.mse_loss(pred_lab, target_lab)\n",
    "    return loss\n",
    "\n",
    "pred = torch.randn(1, 3, 10, 10)\n",
    "pred1 = torch.randn(1, 3, 10, 10)\n",
    "out = lab_color_loss(pred, pred1)\n",
    "out.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48031bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0/1000:   0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# modelValA = torch.load(\"./projects/t2v-gif/models/VQVAE-GIF.pt\", map_location=torch.device('cpu'))\n",
    "# modelA.load_state_dict(modelValA)\n",
    "\n",
    "for each_epoch in range(epochs):\n",
    "    modelA.train()\n",
    "    reconstruct_loss = 0.0\n",
    "    codeb_loss = 0.0\n",
    "    commit_loss = 0.0\n",
    "    vqvaeloss = 0.0\n",
    "    diverse_loss = 0.0\n",
    "    ssim_loss = 0.0\n",
    "    \n",
    "    loop = tqdm(dataloader, f\"{each_epoch}/{epochs}\")\n",
    "    perplexities = []\n",
    "\n",
    "    for X, caption in loop:\n",
    "        X = X.to(device)\n",
    "        # Y = Y.to(device)\n",
    "        \n",
    "        quantized_latents, decoderOut, codebook_loss, commitment_loss, encoding_indices, perplexity, diversity_loss = modelA(X)\n",
    "        \n",
    "        # print(X.shape, decoderOut.shape)\n",
    "        X = rearrange(X, 'b t d h w -> (b t) d h w', b = BATCH_SIZE, t = 40, d = 3, h = 128, w = 128)\n",
    "        \n",
    "        ssim_score = ssim(X, decoderOut, data_range=1.0)\n",
    "        ssim_loss = 1.0 - ssim_score\n",
    "\n",
    "        # reconstruction_loss = torch.mean((X - decoderOut)**2)\n",
    "        reconstruction_loss = Fn.l1_loss(decoderOut, X)\n",
    "        colorLoss = lab_color_loss(decoderOut, X)\n",
    "        perceptualoss = perceptualLoss(decoderOut, X)\n",
    "        \n",
    "        loss = reconstruction_loss + codebook_loss + 0.2 * commitment_loss + 0.1 * diversity_loss + 0.1 * ssim_loss + 0.1 * perceptualoss + 0.1 * colorLoss\n",
    "        vqvaeloss += loss.item()\n",
    "\n",
    "        \n",
    "        reconstruct_loss += reconstruction_loss.item()\n",
    "        diverse_loss += diversity_loss.item()\n",
    "        codeb_loss += codebook_loss.item()\n",
    "        commit_loss += commitment_loss.item()\n",
    "        ssim_loss += ssim_loss.item()\n",
    "        perplexities.append(perplexity)\n",
    "        \n",
    "        \n",
    "        optimizerA.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(modelA.parameters(), max_norm=1.0)\n",
    "        optimizerA.step()\n",
    "        loop.set_postfix({\n",
    "            \"TotalL\": f\"{vqvaeloss}\", \n",
    "            \"ReconsL\": f\"{reconstruct_loss}\", \n",
    "            \"CodeL\":f\"{codeb_loss}\",\n",
    "            \"CommitL\":f\"{commitment_loss}\", \n",
    "            \"Perplexity\":f\"{perplexity}\", \n",
    "            \"Diversity Loss\":f\"{diverse_loss}\", \n",
    "            \"SSIM Loss\":f\"{ssim_loss}\",\n",
    "            \"Perceptual Loss\":f\"{perceptualoss}\",\n",
    "            \"Color Loss\":f\"{colorLoss}\"\n",
    "        })\n",
    "    #     break\n",
    "    # break\n",
    "\n",
    "    average_perplexity = sum(perplexities)/len(perplexities)\n",
    "    vqvaeloss /= len(dataloader)   \n",
    "    reconstruct_loss /= len(dataloader)   \n",
    "    codeb_loss /= len(dataloader)   \n",
    "    commit_loss /= len(dataloader)   \n",
    "    diverse_loss /= len(dataloader)\n",
    "    perceptualoss /= len(dataloader)\n",
    "    colorLoss /= len(dataloader)\n",
    "    torch.save(modelA.state_dict(), \"./models/VQVAE-GIF.pt\")\n",
    "    wandb.log({\n",
    "        \"VQVAE LR\": optimizerA.param_groups[0]['lr'],\n",
    "        \"VQVAE Loss\": vqvaeloss,\n",
    "        \"Reconstruction Loss\": reconstruct_loss,\n",
    "        \"Codebook Loss\": codeb_loss,\n",
    "        \"Commitment Loss\": commit_loss,\n",
    "        \"Diversity Loss\": diverse_loss,\n",
    "        \"Perplexity\": average_perplexity,\n",
    "        \"SSIM Loss\":ssim_loss,\n",
    "        \"Perceptual Loss\":perceptualoss,\n",
    "        \"Color Loss\":colorLoss\n",
    "    })\n",
    "    schedulerA.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afbfe8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
