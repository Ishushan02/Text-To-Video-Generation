{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b646838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional  as Fn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from io import BytesIO\n",
    "from IPython.core.display import HTML\n",
    "from IPython.display import Image as IPyImage, display\n",
    "from PIL import Image, ImageSequence\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from piq import ssim\n",
    "import pandas as pd\n",
    "import urllib\n",
    "from torchvision.models import vgg16\n",
    "import io\n",
    "import cv2\n",
    "import hashlib\n",
    "import kornia\n",
    "\n",
    "# import wandb\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dffb08ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(\n",
    "#     project=\"Transformer-Decoder\",  \n",
    "#     name=\"experiment-2\",    \n",
    "#     # id=\"uqcub7jq\",  \n",
    "#     # resume=\"allow\",\n",
    "#     # config={                       \n",
    "#     #     \"epochs\": 1000,\n",
    "#     #     \"batch_size\": 64,\n",
    "#     # }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "967b1300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.3374e-02, -2.4344e-01,  9.7756e-02,  ...,  9.8517e-02,\n",
       "           1.0220e-01, -2.1899e-01],\n",
       "         [-1.2528e-01,  9.2228e-02, -1.5133e-01,  ..., -6.6822e-03,\n",
       "           2.3273e-01,  1.0878e-01],\n",
       "         [-2.2598e-01,  2.3129e-01, -1.4342e-01,  ...,  2.2462e-01,\n",
       "          -2.3867e-01, -1.8427e-01],\n",
       "         ...,\n",
       "         [ 1.9732e-01,  1.4822e-01, -7.5449e-02,  ..., -1.0177e-02,\n",
       "          -1.8945e-01, -3.1792e-02],\n",
       "         [ 1.2054e-01, -2.3742e-01,  1.6465e-01,  ..., -1.7200e-01,\n",
       "          -8.2114e-02, -2.1017e-04],\n",
       "         [ 1.9834e-01,  1.6758e-02,  5.6952e-02,  ...,  2.2517e-01,\n",
       "          -6.4743e-03, -1.5757e-01]], grad_fn=<MmBackward0>),\n",
       " tensor([26, 18, 63,  ..., 33, 37, 58]),\n",
       " tensor(59.9182),\n",
       " tensor(0.0158))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorQuantizeImage(nn.Module):\n",
    "    def __init__(self, codeBookDim = 64, embeddingDim = 32, decay = 0.99, eps = 1e-5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.codeBookDim = codeBookDim\n",
    "        self.embeddingDim = embeddingDim\n",
    "        self.decay = decay\n",
    "        self.eps = eps\n",
    "        self.dead_codeBook_threshold = codeBookDim * 0.6\n",
    "\n",
    "        self.codebook = nn.Embedding(codeBookDim, embeddingDim)\n",
    "        nn.init.xavier_uniform_(self.codebook.weight.data)\n",
    "\n",
    "        self.register_buffer('ema_Count', torch.zeros(codeBookDim))\n",
    "        self.register_buffer('ema_Weight', self.codebook.weight.data.clone())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_reshaped = x.view(-1, self.embeddingDim)\n",
    "\n",
    "        distance = (torch.sum(x_reshaped**2, dim=1, keepdim=True) \n",
    "                    + torch.sum(self.codebook.weight**2, dim=1)\n",
    "                    - 2 * torch.matmul(x_reshaped, self.codebook.weight.t()))\n",
    "        \n",
    "        encoding_indices = torch.argmin(distance, dim=1) \n",
    "        encodings = Fn.one_hot(encoding_indices, self.codeBookDim).type(x_reshaped.dtype)\n",
    "        quantized = torch.matmul(encodings, self.codebook.weight)\n",
    "\n",
    "        if self.training:\n",
    "            self.ema_Count = self.decay * self.ema_Count + (1 - self.decay) * torch.sum(encodings, 0)\n",
    "            \n",
    "            x_reshaped_sum = torch.matmul(encodings.t(), x_reshaped.detach())\n",
    "            self.ema_Weight = self.decay * self.ema_Weight + (1 - self.decay) * x_reshaped_sum\n",
    "            \n",
    "            n = torch.clamp(self.ema_Count, min=self.eps)\n",
    "            updated_embeddings = self.ema_Weight / n.unsqueeze(1)\n",
    "            self.codebook.weight.data.copy_(updated_embeddings)\n",
    "\n",
    "        \n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        log_encoding_sum = -torch.sum(avg_probs * torch.log(avg_probs + 1e-10))\n",
    "        perplexity = torch.exp(log_encoding_sum)\n",
    "\n",
    "        entropy = log_encoding_sum\n",
    "        normalized_entropy = entropy / torch.log(torch.tensor(self.codeBookDim, device=x.device))\n",
    "        diversity_loss = 1.0 - normalized_entropy\n",
    "\n",
    "        return quantized, encoding_indices, perplexity, diversity_loss\n",
    "        \n",
    "        \n",
    "vq = VectorQuantizeImage(codeBookDim=64,embeddingDim=32)\n",
    "rand = torch.randn(1024,32)\n",
    "vq(rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eff123c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81920"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*10*256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51095fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([320, 64, 16, 16]),\n",
       " torch.Size([320, 3, 64, 64]),\n",
       " tensor(0.1738, grad_fn=<MseLossBackward0>),\n",
       " tensor(0.1738, grad_fn=<MseLossBackward0>),\n",
       " torch.Size([81920]),\n",
       " tensor(46.8948),\n",
       " tensor(0.2069))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VecQVAE(nn.Module):\n",
    "    def __init__(self, inChannels = 1, hiddenDim = 32, codeBookdim = 128, embedDim = 128):\n",
    "        super().__init__()\n",
    "        self.inChannels = inChannels\n",
    "        self.hiddenDim = hiddenDim\n",
    "        self.codeBookdim = codeBookdim\n",
    "        self.embedDim = embedDim\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(inChannels, hiddenDim, 4, 2, 1), \n",
    "            nn.BatchNorm2d(hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(hiddenDim, hiddenDim, 3, 1, 1),\n",
    "            nn.BatchNorm2d(hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(hiddenDim, 2 * hiddenDim, 4, 2, 1),\n",
    "            nn.BatchNorm2d(2 * hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(2 * hiddenDim, 2 * hiddenDim, 3, 1, 1),\n",
    "            nn.BatchNorm2d(2 * hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(2 * hiddenDim, embedDim, 1),\n",
    "        )\n",
    "\n",
    "        self.vector_quantize = VectorQuantizeImage(codeBookDim=codeBookdim,embeddingDim=embedDim)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(embedDim, 2 * hiddenDim, 4, 2, 1),\n",
    "            nn.BatchNorm2d(2 * hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(2 * hiddenDim, 2 * hiddenDim, 3, 1, 1),\n",
    "            nn.BatchNorm2d(2 * hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(2 * hiddenDim, hiddenDim, 4, 2, 1),\n",
    "            nn.BatchNorm2d(hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(hiddenDim, hiddenDim, 3, 1, 1),\n",
    "            nn.BatchNorm2d(hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(hiddenDim, inChannels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encodeImage(self, x, noise_std = 0.15):\n",
    "        if self.training:\n",
    "            encodedOut = self.encoder(x)\n",
    "            encodedOut = encodedOut + torch.randn_like(encodedOut) * noise_std\n",
    "        else:\n",
    "            encodedOut = self.encoder(x)\n",
    "\n",
    "        return encodedOut\n",
    "\n",
    "    def decodeImage(self, quantized_vector):\n",
    "        decodedOut = self.decoder(quantized_vector)\n",
    "        return decodedOut\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, time_frame, inChannels, height, width = x.shape\n",
    "\n",
    "        x_frames = rearrange(x, 'b t c h w -> (b t) c h w')\n",
    "        encodedOut = self.encodeImage(x_frames)\n",
    "        batch_size_time_frame, encoded_channel, encoded_height, encoded_width = encodedOut.shape\n",
    "        \n",
    "        # print(f\"Encoded Shape: {encodedOut.shape}\")\n",
    "\n",
    "        \n",
    "        vectorize_input = rearrange(encodedOut, 'bt d h w -> (bt h w) d')\n",
    "        quantized_vectors, encoding_indices, perplexity, diversity_loss  = self.vector_quantize(vectorize_input)\n",
    "        codebook_loss = Fn.mse_loss(vectorize_input.detach(), quantized_vectors)\n",
    "        commitment_loss = Fn.mse_loss(vectorize_input, quantized_vectors.detach())\n",
    "\n",
    "        quantized_vectors = vectorize_input + (quantized_vectors - vectorize_input).detach()\n",
    "        # print(f\"CodeBook Loss: {codebook_loss} , Commitment Loss: {commitment_loss}\")\n",
    "        # print(f\"Quantized SHape: {quantized_vectors.shape}\")\n",
    "\n",
    "        decoder_input = rearrange(quantized_vectors, '(bt h w) d -> bt d h w', bt = batch_size_time_frame, d = encoded_channel, h = encoded_height, w = encoded_width)\n",
    "        # print(f\"Decoded Input SHape: {decoder_input.shape}\")\n",
    "        decodedOut = self.decodeImage(decoder_input)\n",
    "\n",
    "        # print(f\"Decoded SHape: {decodedOut.shape}\")\n",
    "        \n",
    "        return decoder_input, decodedOut, codebook_loss, commitment_loss, encoding_indices, perplexity, diversity_loss\n",
    "\n",
    "VQ = VecQVAE(inChannels = 3, hiddenDim = 256, codeBookdim = 128, embedDim = 64)\n",
    "test = torch.randn(32, 10, 3, 64, 64)\n",
    "quantized_latents, decoderOut, codebook_loss, commitment_loss, encoding_indices, perplexity, diversity_loss = VQ(test)\n",
    "quantized_latents.shape, decoderOut.shape, codebook_loss, commitment_loss, encoding_indices.shape, perplexity, diversity_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2d40a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 128, 64, 64])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SelfAttentionBlock(nn.Module):\n",
    "    def __init__(self, inChannels, heads = 8):\n",
    "        super().__init__()\n",
    "        self.query = nn.Conv2d(inChannels, inChannels // heads, 1)\n",
    "        self.key = nn.Conv2d(inChannels, inChannels // heads, 1)\n",
    "        self.value = nn.Conv2d(inChannels, inChannels, 1)\n",
    "\n",
    "        self.coeff = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, channel, height, width = x.shape\n",
    "        q = self.query(x).view(batch, -1, height * width).permute(0, 2, 1)\n",
    "        k = self.key(x).view(batch, -1, height * width)\n",
    "        v = self.value(x).view(batch, -1, height * width)\n",
    "        attn = torch.matmul(q, k)                                          \n",
    "        attn = Fn.softmax(attn, dim=-1)\n",
    "        attn_reshaped = attn.permute(0, 2, 1) \n",
    "        \n",
    "        out = torch.matmul(v, attn_reshaped)                        \n",
    "        out = out.view(batch, channel, height, width)                    \n",
    "        out = self.coeff * out + x                                      \n",
    "        return out\n",
    "\n",
    "rad = torch.randn(10, 128, 64, 64)\n",
    "sAtt = SelfAttentionBlock(128, 4)\n",
    "out = sAtt(rad)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8eeb6967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 128, 16, 16]),\n",
       " torch.Size([10, 3, 64, 64]),\n",
       " tensor(0.1665, grad_fn=<MseLossBackward0>),\n",
       " tensor(0.1665, grad_fn=<MseLossBackward0>),\n",
       " torch.Size([2560]),\n",
       " tensor(271.1230),\n",
       " tensor(0.1917))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VecQVAE(nn.Module):\n",
    "    def __init__(self, inChannels=3, hiddenDim=256, codeBookdim=256, embedDim=128):\n",
    "        super().__init__()\n",
    "        self.inChannels = inChannels\n",
    "        self.hiddenDim = hiddenDim\n",
    "        self.codeBookdim = codeBookdim\n",
    "        self.embedDim = embedDim\n",
    "\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(inChannels, hiddenDim, 4, 2, 1),\n",
    "            nn.BatchNorm2d(hiddenDim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(hiddenDim, hiddenDim, 3, 1, 1),\n",
    "            nn.BatchNorm2d(hiddenDim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(hiddenDim, 2 * hiddenDim, 4, 2, 1),\n",
    "            nn.BatchNorm2d(2 * hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            SelfAttentionBlock(2 * hiddenDim)\n",
    "        )\n",
    "\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv2d(2 * hiddenDim, 2 * hiddenDim, 3, 1, 1),\n",
    "            nn.BatchNorm2d(2 * hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            SelfAttentionBlock(2 * hiddenDim)\n",
    "        )\n",
    "\n",
    "        self.block5 = nn.Sequential(\n",
    "            nn.Conv2d(2 * hiddenDim, embedDim, 1)\n",
    "        )\n",
    "\n",
    "        self.vector_quantize = VectorQuantizeImage(codeBookDim=codeBookdim, embeddingDim=embedDim)\n",
    "\n",
    "        self.block6 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(embedDim, 2 * hiddenDim, 1),\n",
    "            nn.BatchNorm2d(2 * hiddenDim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.block7 = nn.Sequential(\n",
    "            nn.Conv2d(2 * hiddenDim, 2 * hiddenDim, 3, 1, 1),\n",
    "            nn.BatchNorm2d(2 * hiddenDim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.block8 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2 * hiddenDim, hiddenDim, 4, 2, 1),\n",
    "            nn.BatchNorm2d(hiddenDim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.block9 = nn.Sequential(\n",
    "            nn.Conv2d(hiddenDim, hiddenDim, 3, 1, 1),\n",
    "            nn.BatchNorm2d(hiddenDim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.block10 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(hiddenDim, hiddenDim // 2, 4, 2, 1),\n",
    "            nn.BatchNorm2d(hiddenDim // 2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "       \n",
    "        self.outputlayer = nn.Sequential(\n",
    "            nn.Conv2d(hiddenDim // 2, inChannels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encodeImage(self, x, noise_std=0.15):\n",
    "        if self.training:\n",
    "            x1 = self.block1(x)\n",
    "            x2 = self.block2(x1)\n",
    "            x3 = self.block3(x2)\n",
    "            x4 = self.block4(x3)\n",
    "            encoded = self.block5(x4)\n",
    "            encoded += torch.randn_like(encoded) * noise_std\n",
    "        else:\n",
    "            x1 = self.block1(x)\n",
    "            x2 = self.block2(x1)\n",
    "            x3 = self.block3(x2)\n",
    "            x4 = self.block4(x3)\n",
    "            encoded = self.block5(x4)\n",
    "        return encoded, (x2, x3, x4)\n",
    "\n",
    "    def decodeImage(self, quantized_vector, skips):\n",
    "        x2, x3, x4 = skips\n",
    "        # print(x2.shape, x3.shape, x4.shape)\n",
    "        x = self.block6(quantized_vector)\n",
    "        x = self.block7(x + x4)\n",
    "        x = self.block8(x + x3)\n",
    "        x = self.block9(x + x2)\n",
    "        x = self.block10(x)\n",
    "        return self.outputlayer(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, timeFrames, channel, height, width = x.shape\n",
    "        x = rearrange(x, 'b t c h w -> (b t) c h w')\n",
    "\n",
    "        encoded, skips = self.encodeImage(x)\n",
    "        batchTime, encodedChannels, encodedHeight, encodedWidth = encoded.shape\n",
    "        encoder_reshaped = rearrange(encoded, 'bt d h w -> (bt h w) d')\n",
    "        quantized_vectors, encoding_indices, perplexity, diversity_loss = self.vector_quantize(encoder_reshaped)\n",
    "\n",
    "        codebook_loss = Fn.mse_loss(encoder_reshaped.detach(), quantized_vectors)\n",
    "        commitment_loss = Fn.mse_loss(encoder_reshaped, quantized_vectors.detach())\n",
    "\n",
    "        quantized_vectors = encoder_reshaped + (quantized_vectors - encoder_reshaped).detach()\n",
    "        # print(quantized.shape)\n",
    "        decoder_input = rearrange(quantized_vectors, '(bt h w) d -> bt d h w', bt=batch * timeFrames, d=encodedChannels, h=encodedHeight, w=encodedWidth)\n",
    "        # print(decoder_input.shape)\n",
    "\n",
    "        decodedOut= self.decodeImage(decoder_input, skips)\n",
    "        return decoder_input, decodedOut, codebook_loss, commitment_loss, encoding_indices, perplexity, diversity_loss\n",
    "\n",
    "VQ = VecQVAE(inChannels = 3, hiddenDim = 512, codeBookdim = 1024, embedDim = 128)\n",
    "test = torch.randn(1, 10, 3, 64, 64)\n",
    "quantized_latents, decoderOut, codebook_loss, commitment_loss, encoding_indices, perplexity, diversity_loss = VQ(test)\n",
    "quantized_latents.shape, decoderOut.shape, codebook_loss, commitment_loss, encoding_indices.shape, perplexity, diversity_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7860d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78568, 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"/Users/ishananand/Desktop/Text-To-Video-Generation/data/modified_tgif.csv\")\n",
    "dataset = dataset[(dataset['frames'] <= 40) & (dataset['frames'] > 15)].copy().reset_index(drop=True)\n",
    "# dataset = dataset[:10000] # thread 1 first 10000\n",
    "\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6aa29ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(377, 500, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getNumpyArray(dataset, index):\n",
    "    url = dataset['url'][index]\n",
    "    resp = urllib.request.urlopen(url)\n",
    "    image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
    "    image = cv2.imdecode(image, cv2.IMREAD_COLOR_RGB)\n",
    "\n",
    "    return image\n",
    "\n",
    "tImg = getNumpyArray(dataset, 10)\n",
    "tImg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4430799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 281, 500, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getNumpyArray(dataset, index):\n",
    "    url = dataset.iloc[index]['url']\n",
    "    resp = urllib.request.urlopen(url)\n",
    "    data = resp.read()\n",
    "    pil_image = Image.open(io.BytesIO(data))\n",
    "    \n",
    "    frames = []\n",
    "    for frame in ImageSequence.Iterator(pil_image):\n",
    "        frame = frame.convert(\"RGB\")\n",
    "        frame_np = np.array(frame)\n",
    "        frames.append(frame_np)\n",
    "    \n",
    "    frames = np.array(frames)\n",
    "    return frames\n",
    "\n",
    "\n",
    "tImg = getNumpyArray(dataset, 2000)\n",
    "tImg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cecd366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a man is shaking a dog's hand. w\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://33.media.tumblr.com/b2b7aa8727c11a72e3ac761e80d8f19d/tumblr_nqlx8zU9yJ1uzvurno1_250.gif\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset['caption'][2222])\n",
    "HTML(f'<img src=\"{dataset['url'][2222]}\" />')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "060176de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 3, 256, 256]) a man dressed in red is dancing.\n"
     ]
    }
   ],
   "source": [
    "class FrameDataset(Dataset):\n",
    "    def __init__(self, data, totalSequence = 40, transform = None):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.totalSequence = totalSequence\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def npArray(self, index):\n",
    "        try:\n",
    "            row = self.data.iloc[index]\n",
    "            totalframes = self.data.iloc[index]['frames']\n",
    "            url = row['url']\n",
    "            resp = urllib.request.urlopen(url)\n",
    "            image_data = resp.read()\n",
    "            img = Image.open(io.BytesIO(image_data))\n",
    "    \n",
    "            frames = []\n",
    "            for frame in ImageSequence.Iterator(img):\n",
    "                frame_rgb = frame.convert(\"RGB\")\n",
    "                frames.append(np.array(frame_rgb))\n",
    "    \n",
    "            return frames\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing index {index}: {e}\")\n",
    "            fallback = torch.zeros((256, 256, 3), dtype=torch.uint8)\n",
    "            return [fallback.numpy()]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # print(index)\n",
    "        gif = self.npArray(index)\n",
    "        caption = self.data.iloc[index]['caption']\n",
    "        totalframes = len(gif)#self.data.iloc[index]['frames']\n",
    "        \n",
    "        if totalframes < self.totalSequence:\n",
    "            gif += [gif[-1]] * (self.totalSequence - totalframes)\n",
    "\n",
    "        tensorFrames = torch.stack([\n",
    "            self.transform(Image.fromarray(frame)) for frame in gif\n",
    "        ])\n",
    "\n",
    "        tensorFrames = tensorFrames/255.0\n",
    "\n",
    "        return tensorFrames, caption\n",
    "    \n",
    "\n",
    "tranform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "fdata = FrameDataset(dataset, transform=tranform)\n",
    "\n",
    "X, Y = fdata.__getitem__(1)\n",
    "print(X.shape, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "66520a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 3, 256, 256]) a man dressed in red is dancing.\n"
     ]
    }
   ],
   "source": [
    "class FrameDataset(Dataset):\n",
    "    def __init__(self, data, totalSequence=40, transform=None, cache_dir='/Users/ishananand/Desktop/Text-To-Video-Generation/data/cacheGIF'):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.totalSequence = totalSequence\n",
    "        self.cache_dir = cache_dir\n",
    "        os.makedirs(self.cache_dir, exist_ok=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def cache_path(self, index):\n",
    "        url = self.data.iloc[index]['url']\n",
    "        hash_name = hashlib.md5(url.encode()).hexdigest()\n",
    "        return os.path.join(self.cache_dir, f'{hash_name}.pt')\n",
    "\n",
    "    def npArray(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        url = row['url']\n",
    "        resp = urllib.request.urlopen(url)\n",
    "        image_data = resp.read()\n",
    "        img = Image.open(io.BytesIO(image_data))\n",
    "\n",
    "        frames = []\n",
    "        for frame in ImageSequence.Iterator(img):\n",
    "            frame_rgb = frame.convert(\"RGB\")\n",
    "            frames.append(np.array(frame_rgb))\n",
    "        return frames\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        totalframes = self.data.iloc[index]['frames']\n",
    "        caption = self.data.iloc[index]['caption']\n",
    "\n",
    "        cache_path = self.cache_path(index)\n",
    "\n",
    "        if os.path.exists(cache_path):\n",
    "            tensorFrames = torch.load(cache_path)\n",
    "            return tensorFrames, caption\n",
    "\n",
    "        gif = self.npArray(index)\n",
    "        caption = self.data.iloc[index]['caption']\n",
    "        totalframes = self.data.iloc[index]['frames']\n",
    "\n",
    "        if totalframes < self.totalSequence:\n",
    "            gif += [gif[-1]] * (self.totalSequence - totalframes)\n",
    "        \n",
    "\n",
    "        tensorFrames = torch.stack([\n",
    "            self.transform(Image.fromarray(frame)) for frame in gif\n",
    "        ])\n",
    "\n",
    "        tensorFrames = tensorFrames / 255.0\n",
    "        torch.save(tensorFrames, cache_path)\n",
    "\n",
    "        return tensorFrames, caption\n",
    "    \n",
    "tranform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "fdata = FrameDataset(dataset, transform=tranform)\n",
    "\n",
    "X, Y = fdata.__getitem__(1)\n",
    "print(X.shape, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81c0a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 3, 256, 256]) a woman is laughing and holding a man, the man is not laughing\n"
     ]
    }
   ],
   "source": [
    "class FrameDataset(Dataset):\n",
    "    def __init__(self, data, totalSequence=40, transform=None, cache_dir='/Users/ishananand/Desktop/Text-To-Video-Generation/data/cacheGIF'):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.totalSequence = totalSequence\n",
    "        self.cache_dir = cache_dir\n",
    "        os.makedirs(self.cache_dir, exist_ok=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def npArray(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        url = row['url']\n",
    "        resp = urllib.request.urlopen(url)\n",
    "        image_data = resp.read()\n",
    "        img = Image.open(io.BytesIO(image_data))\n",
    "\n",
    "        frames = []\n",
    "        for frame in ImageSequence.Iterator(img):\n",
    "            frame_rgb = frame.convert(\"RGB\")\n",
    "            frames.append(np.array(frame_rgb))\n",
    "        return frames\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        url = row['url']\n",
    "        caption = row['caption']\n",
    "        totalframes = row['frames']\n",
    "        gif_path = os.path.join(self.cache_dir, f'{index}.gif')\n",
    "\n",
    "        if not os.path.exists(gif_path):\n",
    "            resp = urllib.request.urlopen(url)\n",
    "            image_data = resp.read()\n",
    "            with open(gif_path, 'wb') as f:\n",
    "                f.write(image_data)\n",
    "        else:\n",
    "            with open(gif_path, 'rb') as f:\n",
    "                image_data = f.read()\n",
    "\n",
    "        img = Image.open(io.BytesIO(image_data))\n",
    "\n",
    "        frames = []\n",
    "        for frame in ImageSequence.Iterator(img):\n",
    "            frame_rgb = frame.convert(\"RGB\")\n",
    "            frames.append(np.array(frame_rgb))\n",
    "\n",
    "        if len(frames) < self.totalSequence:\n",
    "            frames += [frames[-1]] * (self.totalSequence - len(frames))\n",
    "        else:\n",
    "            frames = frames[:self.totalSequence]\n",
    "\n",
    "        if self.transform:\n",
    "            tensorFrames = torch.stack([\n",
    "                self.transform(Image.fromarray(frame)) for frame in frames\n",
    "            ])\n",
    "            tensorFrames = tensorFrames / 255.0\n",
    "            return tensorFrames, caption\n",
    "        else:\n",
    "            return frames, caption\n",
    "\n",
    "    \n",
    "tranform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "fdata = FrameDataset(dataset, transform=tranform)\n",
    "\n",
    "X, Y = fdata.__getitem__(10)\n",
    "print(X.shape, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "38f169c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78568"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72c96eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threads = []\n",
    "i = 0\n",
    "while i < dataset.shape[0]:\n",
    "    threads.append(dataset[i:i+10000])\n",
    "    i = i + 10000\n",
    "\n",
    "threads[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9539214c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13104, 3)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cummulativeIndices = []\n",
    "\n",
    "for i in range(len(threads)):\n",
    "    indices = threads[i].index[threads[i]['frames'] > 35].tolist()\n",
    "    cummulativeIndices.extend(indices)\n",
    "\n",
    "print(len(cummulativeIndices))\n",
    "cummulativeData = dataset.loc[cummulativeIndices]\n",
    "cummulativeData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "33a16a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>caption</th>\n",
       "      <th>frames</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>https://38.media.tumblr.com/88bd6013c943b41c93...</td>\n",
       "      <td>a man with lights on his jacket watching a lar...</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>https://38.media.tumblr.com/88976e0d8b0e068ebd...</td>\n",
       "      <td>the clouds are moving next to the full moon</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>https://38.media.tumblr.com/70f4c1fc63e69bafb2...</td>\n",
       "      <td>a girl in a football uniform is dancing and si...</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>https://38.media.tumblr.com/6fb91b85be51fd128b...</td>\n",
       "      <td>a man with longish hair laughs then puts his h...</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>https://38.media.tumblr.com/819a74539497ae1899...</td>\n",
       "      <td>a man are doing acrobatic stunts on the wing o...</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  url  \\\n",
       "11  https://38.media.tumblr.com/88bd6013c943b41c93...   \n",
       "15  https://38.media.tumblr.com/88976e0d8b0e068ebd...   \n",
       "34  https://38.media.tumblr.com/70f4c1fc63e69bafb2...   \n",
       "40  https://38.media.tumblr.com/6fb91b85be51fd128b...   \n",
       "56  https://38.media.tumblr.com/819a74539497ae1899...   \n",
       "\n",
       "                                              caption  frames  \n",
       "11  a man with lights on his jacket watching a lar...      36  \n",
       "15        the clouds are moving next to the full moon      38  \n",
       "34  a girl in a football uniform is dancing and si...      38  \n",
       "40  a man with longish hair laughs then puts his h...      36  \n",
       "56  a man are doing acrobatic stunts on the wing o...      37  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cummulativeData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2496a18d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No axis named 13123 for object type DataFrame",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(cls, axis)\u001b[39m\n\u001b[32m    576\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m cls._AXIS_TO_AXIS_NUMBER[axis]\n\u001b[32m    577\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m KeyError:\n\u001b[32m--> \u001b[39m\u001b[32m578\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m ValueError(f\"No axis named {axis} for object type {cls.__name__}\")\n",
      "\u001b[31mKeyError\u001b[39m: 13123",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[32m/var/folders/y8/q2s37ndx6tg3lpzp3vp8xk_r0000gn/T/ipykernel_11882/289810394.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m cummulativeData.loc(\u001b[32m13123\u001b[39m)\n",
      "\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/pandas/core/indexing.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, axis)\u001b[39m\n\u001b[32m    734\u001b[39m         \u001b[38;5;66;03m# we need to return a copy of ourselves\u001b[39;00m\n\u001b[32m    735\u001b[39m         new_self = type(self)(self.name, self.obj)\n\u001b[32m    736\u001b[39m \n\u001b[32m    737\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m738\u001b[39m             axis_int_none = self.obj._get_axis_number(axis)\n\u001b[32m    739\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    740\u001b[39m             axis_int_none = axis\n\u001b[32m    741\u001b[39m         new_self.axis = axis_int_none\n",
      "\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(cls, axis)\u001b[39m\n\u001b[32m    574\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m _get_axis_number(cls, axis: Axis) -> AxisInt:\n\u001b[32m    575\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    576\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m cls._AXIS_TO_AXIS_NUMBER[axis]\n\u001b[32m    577\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m KeyError:\n\u001b[32m--> \u001b[39m\u001b[32m578\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m ValueError(f\"No axis named {axis} for object type {cls.__name__}\")\n",
      "\u001b[31mValueError\u001b[39m: No axis named 13123 for object type DataFrame"
     ]
    }
   ],
   "source": [
    "cummulativeData.loc(13123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d822bb16",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No axis named 13123 for object type DataFrame",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(cls, axis)\u001b[39m\n\u001b[32m    576\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m cls._AXIS_TO_AXIS_NUMBER[axis]\n\u001b[32m    577\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m KeyError:\n\u001b[32m--> \u001b[39m\u001b[32m578\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m ValueError(f\"No axis named {axis} for object type {cls.__name__}\")\n",
      "\u001b[31mKeyError\u001b[39m: 13123",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[32m/var/folders/y8/q2s37ndx6tg3lpzp3vp8xk_r0000gn/T/ipykernel_11882/4183035431.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m cummulativeData.iloc(\u001b[32m13123\u001b[39m)\n",
      "\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/pandas/core/indexing.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, axis)\u001b[39m\n\u001b[32m    734\u001b[39m         \u001b[38;5;66;03m# we need to return a copy of ourselves\u001b[39;00m\n\u001b[32m    735\u001b[39m         new_self = type(self)(self.name, self.obj)\n\u001b[32m    736\u001b[39m \n\u001b[32m    737\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m738\u001b[39m             axis_int_none = self.obj._get_axis_number(axis)\n\u001b[32m    739\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    740\u001b[39m             axis_int_none = axis\n\u001b[32m    741\u001b[39m         new_self.axis = axis_int_none\n",
      "\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(cls, axis)\u001b[39m\n\u001b[32m    574\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m _get_axis_number(cls, axis: Axis) -> AxisInt:\n\u001b[32m    575\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    576\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m cls._AXIS_TO_AXIS_NUMBER[axis]\n\u001b[32m    577\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m KeyError:\n\u001b[32m--> \u001b[39m\u001b[32m578\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m ValueError(f\"No axis named {axis} for object type {cls.__name__}\")\n",
      "\u001b[31mValueError\u001b[39m: No axis named 13123 for object type DataFrame"
     ]
    }
   ],
   "source": [
    "cummulativeData(13123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c60e3ec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 15, 34, 40, 56]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cummulativeIndices[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066fabd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13123\n"
     ]
    }
   ],
   "source": [
    "for i in cummulativeIndices:\n",
    "    if i == 13123 :\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5f580c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotFrameCount(dataset):\n",
    "    frameFreq = {}\n",
    "\n",
    "    for i in dataset['frames']:\n",
    "        if i in frameFreq:\n",
    "            frameFreq[i] += 1\n",
    "        else:\n",
    "            frameFreq[i] = 1\n",
    "\n",
    "    plt.figure(figsize=(5, 2))\n",
    "    plt.bar(frameFreq.keys(), frameFreq.values())\n",
    "    plt.xlabel('Frame Count')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Frequency Frame')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bafbc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "codeBookdim = 1024\n",
    "embedDim = 256\n",
    "hiddenDim = 512\n",
    "inChannels = 3\n",
    "tranform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "torchDataset = FrameDataset(dataset, transform=tranform)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dataloader = DataLoader(torchDataset, batch_size=BATCH_SIZE, shuffle = True)\n",
    "modelA = VecQVAE(inChannels = inChannels, hiddenDim = hiddenDim, codeBookdim = codeBookdim, embedDim = embedDim).to(device)\n",
    "lossFn = nn.MSELoss()\n",
    "optimizerA = torch.optim.Adam(\n",
    "                [\n",
    "                    {'params': modelA.parameters(), 'lr': 2e-4},\n",
    "                    # {'params': modelA.decodeImage.parameters(), 'lr': 2e-4},\n",
    "                    # {'params': modelA.vector_quantize.parameters(), 'lr': 1e-4}\n",
    "                ], weight_decay=1e-5)\n",
    "schedulerA = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                optimizerA, T_0=10, T_mult=2, eta_min=1e-6\n",
    "            )\n",
    "\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d743081",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ishananand/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/ishananand/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m pred = torch.randn(\u001b[32m1\u001b[39m, \u001b[32m10\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m10\u001b[39m, \u001b[32m10\u001b[39m)\n\u001b[32m     25\u001b[39m pred1 = torch.randn(\u001b[32m1\u001b[39m, \u001b[32m10\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m10\u001b[39m, \u001b[32m10\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m out = \u001b[43mperceptualLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m out.item()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mperceptualLoss\u001b[39m\u001b[34m(pred, target)\u001b[39m\n\u001b[32m      5\u001b[39m     param.requires_grad = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# print(pred.shape)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m batch, channels, height, width = pred.shape\n\u001b[32m     10\u001b[39m pred = pred.view(batch, channels, height, width)\n\u001b[32m     11\u001b[39m target = target.view(batch, channels, height, width)\n",
      "\u001b[31mValueError\u001b[39m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "def perceptualLoss(pred, target):\n",
    "    vgg = vgg16(pretrained = True).features[:17].eval()\n",
    "    vgg.to(device)\n",
    "    for param in vgg.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # print(pred.shape)\n",
    "    batch, channels, height, width = pred.shape\n",
    "\n",
    "    pred = pred.view(batch, channels, height, width)\n",
    "    target = target.view(batch, channels, height, width)\n",
    "\n",
    "    if pred.shape[1] == 1:\n",
    "        pred = pred.repeat(1, 3, 1, 1)\n",
    "        target = target.repeat(1, 3, 1, 1)\n",
    "\n",
    "\n",
    "    vgg_pred = vgg(pred).to(device)\n",
    "    vgg_true = vgg(target).to(device)\n",
    "\n",
    "    perceptualoss = Fn.mse_loss(vgg_pred, vgg_true)\n",
    "    return perceptualoss\n",
    "\n",
    "pred = torch.randn(1, 10, 3, 10, 10)\n",
    "pred1 = torch.randn(1, 10, 3, 10, 10)\n",
    "out = perceptualLoss(pred, pred1)\n",
    "out.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ed20f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19654.966796875"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lab_color_loss(pred, target):\n",
    "    pred_lab = kornia.color.rgb_to_lab(pred)\n",
    "    target_lab = kornia.color.rgb_to_lab(target)\n",
    "    loss = Fn.mse_loss(pred_lab, target_lab)\n",
    "    return loss\n",
    "\n",
    "pred = torch.randn(1, 3, 10, 10)\n",
    "pred1 = torch.randn(1, 3, 10, 10)\n",
    "out = lab_color_loss(pred, pred1)\n",
    "out.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48031bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0/1000:   0%|          | 0/5000 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# modelValA = torch.load(\"./projects/t2v-gif/models/VQVAE-GIF.pt\", map_location=torch.device('cpu'))\n",
    "# modelA.load_state_dict(modelValA)\n",
    "\n",
    "for each_epoch in range(epochs):\n",
    "    modelA.train()\n",
    "    reconstruct_loss = 0.0\n",
    "    codeb_loss = 0.0\n",
    "    commit_loss = 0.0\n",
    "    vqvaeloss = 0.0\n",
    "    diverse_loss = 0.0\n",
    "    ssim_loss = 0.0\n",
    "    \n",
    "    loop = tqdm(dataloader, f\"{each_epoch}/{epochs}\")\n",
    "    perplexities = []\n",
    "\n",
    "    for X, caption in loop:\n",
    "        X = X.to(device)\n",
    "        # Y = Y.to(device)\n",
    "        \n",
    "        quantized_latents, decoderOut, codebook_loss, commitment_loss, encoding_indices, perplexity, diversity_loss = modelA(X)\n",
    "        \n",
    "        # print(X.shape, decoderOut.shape)\n",
    "        X = rearrange(X, 'b t d h w -> (b t) d h w', b = BATCH_SIZE, t = 40, d = 3, h = 128, w = 128)\n",
    "        \n",
    "        ssim_score = ssim(X, decoderOut, data_range=1.0)\n",
    "        ssim_loss = 1.0 - ssim_score\n",
    "\n",
    "        # reconstruction_loss = torch.mean((X - decoderOut)**2)\n",
    "        reconstruction_loss = Fn.l1_loss(decoderOut, X)\n",
    "        colorLoss = lab_color_loss(decoderOut, X)\n",
    "        perceptualoss = perceptualLoss(decoderOut, X)\n",
    "        \n",
    "        loss = reconstruction_loss + codebook_loss + 0.2 * commitment_loss + 0.1 * diversity_loss + 0.1 * ssim_loss + 0.1 * perceptualoss + 0.1 * colorLoss\n",
    "        vqvaeloss += loss.item()\n",
    "\n",
    "        \n",
    "        reconstruct_loss += reconstruction_loss.item()\n",
    "        diverse_loss += diversity_loss.item()\n",
    "        codeb_loss += codebook_loss.item()\n",
    "        commit_loss += commitment_loss.item()\n",
    "        ssim_loss += ssim_loss.item()\n",
    "        perplexities.append(perplexity)\n",
    "        \n",
    "        \n",
    "        optimizerA.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(modelA.parameters(), max_norm=1.0)\n",
    "        optimizerA.step()\n",
    "        loop.set_postfix({\n",
    "            \"TotalL\": f\"{vqvaeloss}\", \n",
    "            \"ReconsL\": f\"{reconstruct_loss}\", \n",
    "            \"CodeL\":f\"{codeb_loss}\",\n",
    "            \"CommitL\":f\"{commitment_loss}\", \n",
    "            \"Perplexity\":f\"{perplexity}\", \n",
    "            \"Diversity Loss\":f\"{diverse_loss}\", \n",
    "            \"SSIM Loss\":f\"{ssim_loss}\",\n",
    "            \"Perceptual Loss\":f\"{perceptualoss}\",\n",
    "            \"Color Loss\":f\"{colorLoss}\"\n",
    "        })\n",
    "    #     break\n",
    "    # break\n",
    "\n",
    "    average_perplexity = sum(perplexities)/len(perplexities)\n",
    "    vqvaeloss /= len(dataloader)   \n",
    "    reconstruct_loss /= len(dataloader)   \n",
    "    codeb_loss /= len(dataloader)   \n",
    "    commit_loss /= len(dataloader)   \n",
    "    diverse_loss /= len(dataloader)\n",
    "    perceptualoss /= len(dataloader)\n",
    "    colorLoss /= len(dataloader)\n",
    "    torch.save(modelA.state_dict(), \"./models/VQVAE-GIF.pt\")\n",
    "    wandb.log({\n",
    "        \"VQVAE LR\": optimizerA.param_groups[0]['lr'],\n",
    "        \"VQVAE Loss\": vqvaeloss,\n",
    "        \"Reconstruction Loss\": reconstruct_loss,\n",
    "        \"Codebook Loss\": codeb_loss,\n",
    "        \"Commitment Loss\": commit_loss,\n",
    "        \"Diversity Loss\": diverse_loss,\n",
    "        \"Perplexity\": average_perplexity,\n",
    "        \"SSIM Loss\":ssim_loss,\n",
    "        \"Perceptual Loss\":perceptualoss,\n",
    "        \"Color Loss\":colorLoss\n",
    "    })\n",
    "    schedulerA.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afbfe8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
