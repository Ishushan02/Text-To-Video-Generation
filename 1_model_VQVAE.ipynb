{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b646838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchview import draw_graph\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional  as Fn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from io import BytesIO\n",
    "from IPython.core.display import HTML\n",
    "from IPython.display import Image as IPyImage, display\n",
    "from PIL import Image, ImageSequence\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from piq import ssim\n",
    "import gc\n",
    "import pandas as pd\n",
    "import urllib\n",
    "import io\n",
    "import cv2\n",
    "import hashlib\n",
    "\n",
    "# import wandb\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dffb08ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(\n",
    "#     project=\"Transformer-Decoder\",  \n",
    "#     name=\"experiment-2\",    \n",
    "#     # id=\"uqcub7jq\",  \n",
    "#     # resume=\"allow\",\n",
    "#     # config={                       \n",
    "#     #     \"epochs\": 1000,\n",
    "#     #     \"batch_size\": 64,\n",
    "#     # }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "967b1300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1592, -0.1883,  0.1970,  ..., -0.1586, -0.0167,  0.2281],\n",
       "         [ 0.0870,  0.0423,  0.2319,  ...,  0.1863,  0.2108,  0.2063],\n",
       "         [-0.0688,  0.1542, -0.0923,  ...,  0.0319, -0.1878, -0.0544],\n",
       "         ...,\n",
       "         [ 0.0928, -0.2319,  0.0472,  ...,  0.1059, -0.1991,  0.1395],\n",
       "         [ 0.1181,  0.1439, -0.1763,  ...,  0.1020, -0.0362,  0.2144],\n",
       "         [ 0.0183, -0.1064,  0.2141,  ..., -0.1721,  0.1392,  0.0845]],\n",
       "        grad_fn=<MmBackward0>),\n",
       " tensor([54, 63, 15,  ..., 32,  0, 11]),\n",
       " tensor(60.3121),\n",
       " tensor(0.0143))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorQuantizeImage(nn.Module):\n",
    "    def __init__(self, codeBookDim = 64, embeddingDim = 32, decay = 0.99, eps = 1e-5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.codeBookDim = codeBookDim\n",
    "        self.embeddingDim = embeddingDim\n",
    "        self.decay = decay\n",
    "        self.eps = eps\n",
    "        self.dead_codeBook_threshold = codeBookDim * 0.6\n",
    "\n",
    "        self.codebook = nn.Embedding(codeBookDim, embeddingDim)\n",
    "        nn.init.xavier_uniform_(self.codebook.weight.data)\n",
    "\n",
    "        self.register_buffer('ema_Count', torch.zeros(codeBookDim))\n",
    "        self.register_buffer('ema_Weight', self.codebook.weight.data.clone())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_reshaped = x.view(-1, self.embeddingDim)\n",
    "\n",
    "        distance = (torch.sum(x_reshaped**2, dim=1, keepdim=True) \n",
    "                    + torch.sum(self.codebook.weight**2, dim=1)\n",
    "                    - 2 * torch.matmul(x_reshaped, self.codebook.weight.t()))\n",
    "        \n",
    "        encoding_indices = torch.argmin(distance, dim=1) \n",
    "        encodings = Fn.one_hot(encoding_indices, self.codeBookDim).type(x_reshaped.dtype)\n",
    "        quantized = torch.matmul(encodings, self.codebook.weight)\n",
    "\n",
    "        if self.training:\n",
    "            self.ema_Count = self.decay * self.ema_Count + (1 - self.decay) * torch.sum(encodings, 0)\n",
    "            \n",
    "            x_reshaped_sum = torch.matmul(encodings.t(), x_reshaped.detach())\n",
    "            self.ema_Weight = self.decay * self.ema_Weight + (1 - self.decay) * x_reshaped_sum\n",
    "            \n",
    "            n = torch.clamp(self.ema_Count, min=self.eps)\n",
    "            updated_embeddings = self.ema_Weight / n.unsqueeze(1)\n",
    "            self.codebook.weight.data.copy_(updated_embeddings)\n",
    "\n",
    "        \n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        log_encoding_sum = -torch.sum(avg_probs * torch.log(avg_probs + 1e-10))\n",
    "        perplexity = torch.exp(log_encoding_sum)\n",
    "\n",
    "        entropy = log_encoding_sum\n",
    "        normalized_entropy = entropy / torch.log(torch.tensor(self.codeBookDim, device=x.device))\n",
    "        diversity_loss = 1.0 - normalized_entropy\n",
    "\n",
    "        return quantized, encoding_indices, perplexity, diversity_loss\n",
    "        \n",
    "        \n",
    "vq = VectorQuantizeImage(codeBookDim=64,embeddingDim=32)\n",
    "rand = torch.randn(1024,32)\n",
    "vq(rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eff123c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81920"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*10*256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51095fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([320, 64, 16, 16]),\n",
       " torch.Size([320, 3, 64, 64]),\n",
       " tensor(0.1746, grad_fn=<MseLossBackward0>),\n",
       " tensor(0.1746, grad_fn=<MseLossBackward0>),\n",
       " torch.Size([81920]),\n",
       " tensor(43.4719),\n",
       " tensor(0.2226))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VecQVAE(nn.Module):\n",
    "    def __init__(self, inChannels = 1, hiddenDim = 32, codeBookdim = 128, embedDim = 128):\n",
    "        super().__init__()\n",
    "        self.inChannels = inChannels\n",
    "        self.hiddenDim = hiddenDim\n",
    "        self.codeBookdim = codeBookdim\n",
    "        self.embedDim = embedDim\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(inChannels, hiddenDim, 4, 2, 1), \n",
    "            nn.BatchNorm2d(hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(hiddenDim, hiddenDim, 3, 1, 1),\n",
    "            nn.BatchNorm2d(hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(hiddenDim, 2 * hiddenDim, 4, 2, 1),\n",
    "            nn.BatchNorm2d(2 * hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(2 * hiddenDim, 2 * hiddenDim, 3, 1, 1),\n",
    "            nn.BatchNorm2d(2 * hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(2 * hiddenDim, embedDim, 1),\n",
    "        )\n",
    "\n",
    "        self.vector_quantize = VectorQuantizeImage(codeBookDim=codeBookdim,embeddingDim=embedDim)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(embedDim, 2 * hiddenDim, 4, 2, 1),\n",
    "            nn.BatchNorm2d(2 * hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(2 * hiddenDim, 2 * hiddenDim, 3, 1, 1),\n",
    "            nn.BatchNorm2d(2 * hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(2 * hiddenDim, hiddenDim, 4, 2, 1),\n",
    "            nn.BatchNorm2d(hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(hiddenDim, hiddenDim, 3, 1, 1),\n",
    "            nn.BatchNorm2d(hiddenDim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(hiddenDim, inChannels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encodeImage(self, x, noise_std = 0.15):\n",
    "        if self.training:\n",
    "            encodedOut = self.encoder(x)\n",
    "            encodedOut = encodedOut + torch.randn_like(encodedOut) * noise_std\n",
    "        else:\n",
    "            encodedOut = self.encoder(x)\n",
    "\n",
    "        return encodedOut\n",
    "\n",
    "    def decodeImage(self, quantized_vector):\n",
    "        decodedOut = self.decoder(quantized_vector)\n",
    "        return decodedOut\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, time_frame, inChannels, height, width = x.shape\n",
    "\n",
    "        x_frames = rearrange(x, 'b t c h w -> (b t) c h w')\n",
    "        encodedOut = self.encodeImage(x_frames)\n",
    "        batch_size_time_frame, encoded_channel, encoded_height, encoded_width = encodedOut.shape\n",
    "        \n",
    "        # print(f\"Encoded Shape: {encodedOut.shape}\")\n",
    "\n",
    "        \n",
    "        vectorize_input = rearrange(encodedOut, 'bt d h w -> (bt h w) d')\n",
    "        quantized_vectors, encoding_indices, perplexity, diversity_loss  = self.vector_quantize(vectorize_input)\n",
    "        codebook_loss = Fn.mse_loss(vectorize_input.detach(), quantized_vectors)\n",
    "        commitment_loss = Fn.mse_loss(vectorize_input, quantized_vectors.detach())\n",
    "\n",
    "        quantized_vectors = vectorize_input + (quantized_vectors - vectorize_input).detach()\n",
    "        # print(f\"CodeBook Loss: {codebook_loss} , Commitment Loss: {commitment_loss}\")\n",
    "        # print(f\"Quantized SHape: {quantized_vectors.shape}\")\n",
    "\n",
    "        decoder_input = rearrange(quantized_vectors, '(bt h w) d -> bt d h w', bt = batch_size_time_frame, d = encoded_channel, h = encoded_height, w = encoded_width)\n",
    "        # print(f\"Decoded Input SHape: {decoder_input.shape}\")\n",
    "        decodedOut = self.decodeImage(decoder_input)\n",
    "\n",
    "        # print(f\"Decoded SHape: {decodedOut.shape}\")\n",
    "        \n",
    "        return decoder_input, decodedOut, codebook_loss, commitment_loss, encoding_indices, perplexity, diversity_loss\n",
    "\n",
    "VQ = VecQVAE(inChannels = 3, hiddenDim = 256, codeBookdim = 128, embedDim = 64)\n",
    "test = torch.randn(32, 10, 3, 64, 64)\n",
    "quantized_latents, decoderOut, codebook_loss, commitment_loss, encoding_indices, perplexity, diversity_loss = VQ(test)\n",
    "quantized_latents.shape, decoderOut.shape, codebook_loss, commitment_loss, encoding_indices.shape, perplexity, diversity_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7860d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"/Users/ishananand/Desktop/Text-To-Video-Generation/data/modified_tgif.csv\")\n",
    "dataset = dataset[(dataset['frames'] <= 40) & (dataset['frames'] > 15)].copy().reset_index(drop=True)\n",
    "dataset = dataset[:10000] # thread 1 first 10000\n",
    "\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aa29ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(377, 500, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getNumpyArray(dataset, index):\n",
    "    url = dataset['url'][index]\n",
    "    resp = urllib.request.urlopen(url)\n",
    "    image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
    "    image = cv2.imdecode(image, cv2.IMREAD_COLOR_RGB)\n",
    "\n",
    "    return image\n",
    "\n",
    "tImg = getNumpyArray(dataset, 10)\n",
    "tImg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4430799",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     13\u001b[39m     frames = np.array(frames)\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m frames\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m tImg = \u001b[43mgetNumpyArray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m74801\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m tImg.shape\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mgetNumpyArray\u001b[39m\u001b[34m(dataset, index)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgetNumpyArray\u001b[39m(dataset, index):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     url = \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m[\u001b[33m'\u001b[39m\u001b[33murl\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      3\u001b[39m     resp = urllib.request.urlopen(url)\n\u001b[32m      4\u001b[39m     data = resp.read()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/pandas/core/indexing.py:1191\u001b[39m, in \u001b[36m_LocationIndexer.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1189\u001b[39m maybe_callable = com.apply_if_callable(key, \u001b[38;5;28mself\u001b[39m.obj)\n\u001b[32m   1190\u001b[39m maybe_callable = \u001b[38;5;28mself\u001b[39m._check_deprecated_callable_usage(key, maybe_callable)\n\u001b[32m-> \u001b[39m\u001b[32m1191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/pandas/core/indexing.py:1752\u001b[39m, in \u001b[36m_iLocIndexer._getitem_axis\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot index by location index with a non-integer key\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1751\u001b[39m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1752\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1754\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obj._ixs(key, axis=axis)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/pandas/core/indexing.py:1685\u001b[39m, in \u001b[36m_iLocIndexer._validate_integer\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1683\u001b[39m len_axis = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.obj._get_axis(axis))\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key >= len_axis \u001b[38;5;129;01mor\u001b[39;00m key < -len_axis:\n\u001b[32m-> \u001b[39m\u001b[32m1685\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33msingle positional indexer is out-of-bounds\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mIndexError\u001b[39m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "def getNumpyArray(dataset, index):\n",
    "    url = dataset.iloc[index]['url']\n",
    "    resp = urllib.request.urlopen(url)\n",
    "    data = resp.read()\n",
    "    pil_image = Image.open(io.BytesIO(data))\n",
    "    \n",
    "    frames = []\n",
    "    for frame in ImageSequence.Iterator(pil_image):\n",
    "        frame = frame.convert(\"RGB\")\n",
    "        frame_np = np.array(frame)\n",
    "        frames.append(frame_np)\n",
    "    \n",
    "    frames = np.array(frames)\n",
    "    return frames\n",
    "\n",
    "\n",
    "tImg = getNumpyArray(dataset, 74801)\n",
    "tImg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cecd366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a man is shaking a dog's hand. w\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://33.media.tumblr.com/b2b7aa8727c11a72e3ac761e80d8f19d/tumblr_nqlx8zU9yJ1uzvurno1_250.gif\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset['caption'][2222])\n",
    "HTML(f'<img src=\"{dataset['url'][2222]}\" />')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "060176de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 3, 128, 128]) a man dressed in red is dancing.\n"
     ]
    }
   ],
   "source": [
    "class FrameDataset(Dataset):\n",
    "    def __init__(self, data, totalSequence = 40, transform = None):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.totalSequence = totalSequence\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def npArray(self, index):\n",
    "        try:\n",
    "            row = self.data.iloc[index]\n",
    "            totalframes = self.data.iloc[index]['frames']\n",
    "            url = row['url']\n",
    "            resp = urllib.request.urlopen(url)\n",
    "            image_data = resp.read()\n",
    "            img = Image.open(io.BytesIO(image_data))\n",
    "    \n",
    "            frames = []\n",
    "            for frame in ImageSequence.Iterator(img):\n",
    "                frame_rgb = frame.convert(\"RGB\")\n",
    "                frames.append(np.array(frame_rgb))\n",
    "    \n",
    "            return frames\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing index {index}: {e}\")\n",
    "            fallback = torch.zeros((256, 256, 3), dtype=torch.uint8)\n",
    "            return [fallback.numpy()]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # print(index)\n",
    "        gif = self.npArray(index)\n",
    "        caption = self.data.iloc[index]['caption']\n",
    "        totalframes = len(gif)#self.data.iloc[index]['frames']\n",
    "        \n",
    "        if totalframes < self.totalSequence:\n",
    "            gif += [gif[-1]] * (self.totalSequence - totalframes)\n",
    "\n",
    "        tensorFrames = torch.stack([\n",
    "            self.transform(Image.fromarray(frame)) for frame in gif\n",
    "        ])\n",
    "\n",
    "        tensorFrames = tensorFrames/255.0\n",
    "\n",
    "        return tensorFrames, caption\n",
    "    \n",
    "\n",
    "tranform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "fdata = FrameDataset(dataset, transform=tranform)\n",
    "\n",
    "X, Y = fdata.__getitem__(1)\n",
    "print(X.shape, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66520a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 3, 128, 128]) a guy holds a drink and looks up\n"
     ]
    }
   ],
   "source": [
    "class FrameDataset(Dataset):\n",
    "    def __init__(self, data, totalSequence=40, transform=None, cache_dir='/Users/ishananand/Desktop/Text-To-Video-Generation/data/cacheGIF'):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.totalSequence = totalSequence\n",
    "        self.cache_dir = cache_dir\n",
    "        os.makedirs(self.cache_dir, exist_ok=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def cache_path(self, index):\n",
    "        url = self.data.iloc[index]['url']\n",
    "        hash_name = hashlib.md5(url.encode()).hexdigest()\n",
    "        return os.path.join(self.cache_dir, f'{hash_name}.pt')\n",
    "\n",
    "    def npArray(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        url = row['url']\n",
    "        resp = urllib.request.urlopen(url)\n",
    "        image_data = resp.read()\n",
    "        img = Image.open(io.BytesIO(image_data))\n",
    "\n",
    "        frames = []\n",
    "        for frame in ImageSequence.Iterator(img):\n",
    "            frame_rgb = frame.convert(\"RGB\")\n",
    "            frames.append(np.array(frame_rgb))\n",
    "        return frames\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        cache_path = self.cache_path(index)\n",
    "\n",
    "        if os.path.exists(cache_path):\n",
    "            tensorFrames, caption = torch.load(cache_path)\n",
    "            return tensorFrames, caption\n",
    "\n",
    "        gif = self.npArray(index)\n",
    "        caption = self.data.iloc[index]['caption']\n",
    "        totalframes = self.data.iloc[index]['frames']\n",
    "\n",
    "        if totalframes < self.totalSequence:\n",
    "            gif += [gif[-1]] * (self.totalSequence - totalframes)\n",
    "        \n",
    "\n",
    "        tensorFrames = torch.stack([\n",
    "            self.transform(Image.fromarray(frame)) for frame in gif\n",
    "        ])\n",
    "\n",
    "        tensorFrames = tensorFrames / 255.0\n",
    "        torch.save((tensorFrames, caption), cache_path)\n",
    "\n",
    "        return tensorFrames, caption\n",
    "    \n",
    "tranform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "fdata = FrameDataset(dataset, transform=tranform)\n",
    "\n",
    "X, Y = fdata.__getitem__(74801)\n",
    "print(X.shape, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bafbc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "codeBookdim = 256\n",
    "embedDim = 128\n",
    "hiddenDim = 256\n",
    "inChannels = 3\n",
    "tranform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "torchDataset = FrameDataset(dataset, transform=tranform)\n",
    "device = torch.device('mps' if torch.cuda.is_available() else 'cpu')\n",
    "dataloader = DataLoader(torchDataset, batch_size=BATCH_SIZE, shuffle = True)\n",
    "modelA = VecQVAE(inChannels = inChannels, hiddenDim = hiddenDim, codeBookdim = codeBookdim, embedDim = embedDim).to(device)\n",
    "lossFn = nn.MSELoss()\n",
    "optimizerA = torch.optim.Adam([\n",
    "                    {'params': modelA.encoder.parameters(), 'lr': 2e-4},\n",
    "                    {'params': modelA.decoder.parameters(), 'lr': 2e-4},\n",
    "                    {'params': modelA.vector_quantize.parameters(), 'lr': 1e-4}\n",
    "                ], weight_decay=1e-5)\n",
    "schedulerA = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                optimizerA, T_0=10, T_mult=2, eta_min=1e-6\n",
    "            )\n",
    "\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76ed20f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.Size([64, 40, 3, 128, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48031bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0/1000:   0%|          | 2/39284 [01:10<382:12:06, 35.03s/it, TotalL=1.1930033564567566, ReconsL=0.5787493884563446, CodeL=0.307669073343277, CommitL=0.2024003565311432, Perplexity=31.03209114074707, Diversity Loss=0.45208239555358887, SSIM Loss=1.9987483024597168]   \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m X = X.to(device)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Y = Y.to(device)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m quantized_latents, decoderOut, codebook_loss, commitment_loss, encoding_indices, perplexity, diversity_loss = \u001b[43mmodelA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# print(X.shape, decoderOut.shape)\u001b[39;00m\n\u001b[32m     23\u001b[39m X = rearrange(X, \u001b[33m'\u001b[39m\u001b[33mb t d h w -> (b t) d h w\u001b[39m\u001b[33m'\u001b[39m, b = BATCH_SIZE, t = \u001b[32m40\u001b[39m, d = \u001b[32m3\u001b[39m, h = \u001b[32m128\u001b[39m, w = \u001b[32m128\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 69\u001b[39m, in \u001b[36mVecQVAE.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     66\u001b[39m batch_size, time_frame, inChannels, height, width = x.shape\n\u001b[32m     68\u001b[39m x_frames = rearrange(x, \u001b[33m'\u001b[39m\u001b[33mb t c h w -> (b t) c h w\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m encodedOut = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencodeImage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_frames\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m batch_size_time_frame, encoded_channel, encoded_height, encoded_width = encodedOut.shape\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# print(f\"Encoded Shape: {encodedOut.shape}\")\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 54\u001b[39m, in \u001b[36mVecQVAE.encodeImage\u001b[39m\u001b[34m(self, x, noise_std)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mencodeImage\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, noise_std = \u001b[32m0.15\u001b[39m):\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m         encodedOut = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m         encodedOut = encodedOut + torch.randn_like(encodedOut) * noise_std\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/torch/nn/modules/conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/pytorch_env/lib/python3.13/site-packages/torch/nn/modules/conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# modelValA = torch.load(\"./projects/t2v-gif/models/VQVAE-GIF.pt\", map_location=torch.device('cpu'))\n",
    "# modelA.load_state_dict(modelValA)\n",
    "\n",
    "for each_epoch in range(epochs):\n",
    "    modelA.train()\n",
    "    reconstruct_loss = 0.0\n",
    "    codeb_loss = 0.0\n",
    "    commit_loss = 0.0\n",
    "    vqvaeloss = 0.0\n",
    "    diverse_loss = 0.0\n",
    "    ssim_loss = 0.0\n",
    "    \n",
    "    loop = tqdm(dataloader, f\"{each_epoch}/{epochs}\")\n",
    "    perplexities = []\n",
    "\n",
    "    for X, caption in loop:\n",
    "        X = X.to(device)\n",
    "        # Y = Y.to(device)\n",
    "        \n",
    "        quantized_latents, decoderOut, codebook_loss, commitment_loss, encoding_indices, perplexity, diversity_loss = modelA(X)\n",
    "\n",
    "        # print(X.shape, decoderOut.shape)\n",
    "        X = rearrange(X, 'b t d h w -> (b t) d h w', b = BATCH_SIZE, t = 40, d = 3, h = 128, w = 128)\n",
    "        \n",
    "        ssim_score = ssim(X, decoderOut, data_range=1.0)\n",
    "        ssim_loss = 1.0 - ssim_score\n",
    "\n",
    "        reconstruction_loss = torch.mean((X - decoderOut)**2)\n",
    "        \n",
    "        loss = reconstruction_loss + codebook_loss + 0.2 * commitment_loss + 0.1 * diversity_loss + 0.1 * ssim_loss\n",
    "        vqvaeloss += loss.item()\n",
    "\n",
    "        \n",
    "        reconstruct_loss += reconstruction_loss.item()\n",
    "        diverse_loss += diversity_loss.item()\n",
    "        codeb_loss += codebook_loss.item()\n",
    "        commit_loss += commitment_loss.item()\n",
    "        ssim_loss += ssim_loss.item()\n",
    "        perplexities.append(perplexity)\n",
    "        \n",
    "        \n",
    "        optimizerA.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(modelA.parameters(), max_norm=1.0)\n",
    "        optimizerA.step()\n",
    "        loop.set_postfix({\"TotalL\": f\"{vqvaeloss}\", \"ReconsL\": f\"{reconstruct_loss}\", \"CodeL\":f\"{codeb_loss}\",\n",
    "                          \"CommitL\":f\"{commitment_loss}\", \"Perplexity\":f\"{perplexity}\", \"Diversity Loss\":f\"{diverse_loss}\", \"SSIM Loss\":f\"{ssim_loss}\"})\n",
    "    #     break\n",
    "    # break\n",
    "\n",
    "    average_perplexity = sum(perplexities)/len(perplexities)\n",
    "    vqvaeloss /= len(dataloader)   \n",
    "    reconstruct_loss /= len(dataloader)   \n",
    "    codeb_loss /= len(dataloader)   \n",
    "    commit_loss /= len(dataloader)   \n",
    "    diverse_loss /= len(dataloader)\n",
    "    torch.save(modelA.state_dict(), \"./models/VQVAE-GIF.pt\")\n",
    "    wandb.log({\n",
    "        \"Epoch\": each_epoch,\n",
    "        \"VQVAE LR\": optimizerA.param_groups[0]['lr'],\n",
    "        \"VQVAE Loss\": vqvaeloss,\n",
    "        \"Reconstruction Loss\": reconstruct_loss,\n",
    "        \"Codebook Loss\": codeb_loss,\n",
    "        \"Commitment Loss\": commit_loss,\n",
    "        \"Diversity Loss\": diverse_loss,\n",
    "        \"Perplexity\": average_perplexity,\n",
    "        \"SSIM Loss\":ssim_loss,\n",
    "    })\n",
    "    schedulerA.step()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afbfe8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
